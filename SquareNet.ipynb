{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arkadijs Slobodkins\n",
    "Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells below contain training data generators, validation data generators, functions to test data, and models designed specifically for training certain types of problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1:Generate intervals so that left points are always = low_bound and right points are in (low_bound, up_bound]. Data points \n",
    "#are generated and stored in the form (low_bound, yi).\n",
    "\n",
    "def IntervalsZero(points, grid):\n",
    "    numIntervals = points - 1\n",
    "    Intervals  = np.zeros([numIntervals, 2])\n",
    "    FInterval = np.zeros(numIntervals)\n",
    "    for i in range(numIntervals):\n",
    "        Intervals[i, 0] = grid[0]\n",
    "        Intervals[i, 1] = grid[i+1]\n",
    "    \n",
    "    for i in range(numIntervals):\n",
    "        FInterval[i] = np.exp(grid[i+1]) - np.exp(grid[0])\n",
    "    return Intervals, FInterval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1:Generate intervals so that left points are always = low_bound and right points are in (low_bound, up_bouns]. Data points \n",
    "#are generated and stored in the form yi, discarding x coordinate x = low_bound. \n",
    "\n",
    "def IntervalsZeroPoint(points, grid):\n",
    "    numIntervals = points - 1\n",
    "    Intervals  = np.zeros([numIntervals, 1])\n",
    "    FInterval = np.zeros(numIntervals)\n",
    "    for i in range(numIntervals):\n",
    "        Intervals[i, 0] = grid[i+1]\n",
    "    \n",
    "    for i in range(numIntervals):\n",
    "        FInterval[i] = np.exp(grid[i+1]) - np.exp(grid[0])\n",
    "    return Intervals, FInterval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Generate evenly spaced intervals of equal size and store both coordinates. \n",
    "\n",
    "def IntervalsRandomEven(numIntervals, width, low_bound, up_bound):\n",
    "   \n",
    "    Intervals  = np.zeros([numIntervals, 2])\n",
    "    FInterval = np.zeros(numIntervals)\n",
    "    stepSize = (up_bound - low_bound)/numIntervals\n",
    "    for i in range(numIntervals):\n",
    "        #Intervals[i, 0] = random.uniform(0.0, 1.0-1.2*width)\n",
    "        #Intervals[i, 1] = Intervals[i,0] + width*(1+ random.uniform(0.0, 0.2))\n",
    "        Intervals[i,0] = low_bound + i*stepSize\n",
    "        Intervals[i, 1] = Intervals[i,0] + width#*(1+ random.uniform(-0.1, 0.1))\n",
    "    for i in range(numIntervals):\n",
    "        FInterval[i] = np.exp(Intervals[i,1]) - np.exp(Intervals[i,0])\n",
    "    return Intervals, FInterval\n",
    "\n",
    "\n",
    "#3. Generate evenly spaced intervals of equal size and store both coordinates. \n",
    "\n",
    "def IntervalsRandomEvenPolynomial(numIntervals, width, low_bound, up_bound, power):\n",
    "   \n",
    "    Intervals  = np.zeros([numIntervals, 2])\n",
    "    FInterval = np.zeros(numIntervals)\n",
    "    stepSize = (up_bound - low_bound)/numIntervals\n",
    "    for i in range(numIntervals):\n",
    "        #Intervals[i, 0] = random.uniform(0.0, 1.0-1.2*width)\n",
    "        #Intervals[i, 1] = Intervals[i,0] + width*(1+ random.uniform(0.0, 0.2))\n",
    "        Intervals[i,0] = low_bound + i*stepSize\n",
    "        Intervals[i, 1] = Intervals[i,0] + width#*(1+ random.uniform(-0.1, 0.1))\n",
    "    for i in range(numIntervals):\n",
    "        FInterval[i] = ((Intervals[i,1])**(power+1) - (Intervals[i,0])**(power+1))/(power+1) \n",
    "    return Intervals, FInterval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Generate all possible combinations of the grid points on the interval [low_bound, up_bound].\n",
    "\n",
    "def IntervalsRandom(points, grid):\n",
    "    numIntervals = int((points - 1)*points/2)\n",
    "    Intervals  = np.zeros([numIntervals, 2])\n",
    "    FInterval = np.zeros(numIntervals)\n",
    "    count = 0\n",
    "    for i in range(points):\n",
    "        for j in range(i+1, points):\n",
    "            Intervals[count, 0] = grid[i]\n",
    "            Intervals[count, 1] = grid[j]\n",
    "            count += 1\n",
    "    for i in range(numIntervals):\n",
    "        FInterval[i] = np.exp(Intervals[i, 1]) - np.exp(Intervals[i,0])\n",
    "    return Intervals, FInterval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom loss function is based on template \n",
    "#https://towardsdatascience.com/how-to-create-a-custom-loss-function-keras-3a89156ec69b\n",
    "\n",
    "def Integral_loss(y_true, y_pred):\n",
    "    L2 = K.sum(abs((y_pred-y_true))**2)\n",
    "    loss =  K.sqrt(L2)/points\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model for training dataset where x coordinate is always = low_bound\n",
    "#and input vector is of length 2.\n",
    "\n",
    "def TrainModelPolynomial(Intervals, FInterval):\n",
    "    model = models.Sequential() \n",
    "    model.add(layers.Dense(64,  kernel_initializer='normal', activation = 'relu',  input_shape=(2, )))\n",
    "    model.add(layers.Dense(64,  kernel_initializer='normal', activation = 'relu', ))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    opt = optimizers.Adam(learning_rate=0.00008)\n",
    "    model.compile(loss= Integral_loss, optimizer= opt)\n",
    "    model.fit(Intervals, FInterval, batch_size = 32,  epochs = 50)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model for training dataset where x coordinate is always = low_bound\n",
    "#and input vector is of length 2.\n",
    "\n",
    "def TrainModelZero(Intervals, FInterval, x_val, y_val):\n",
    "    model = models.Sequential() \n",
    "    model.add(layers.Dense(64,  kernel_initializer='normal', activation = 'relu',  input_shape=(2, )))\n",
    "    model.add(layers.Dense(64,  kernel_initializer='normal', activation = 'relu', ))\n",
    "    model.add(Dense(1))\n",
    "    opt = optimizers.Adam(learning_rate=0.00005)\n",
    "    model.compile(loss= Integral_loss, optimizer= opt)\n",
    "    model.fit(Intervals, FInterval, batch_size = 32,  epochs = 75, validation_data = (x_val, y_val))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model for training dataset where x coordinate is always = low_bound\n",
    "#and input vector is of length 1.\n",
    "\n",
    "def TrainModelZeroPoint(Intervals, FInterval):\n",
    "    model = models.Sequential() \n",
    "    model.add(layers.Dense(64,  kernel_initializer='normal', activation = 'relu',  input_shape=(1, )))\n",
    "    model.add(Dense(1))\n",
    "    opt = optimizers.Adam(learning_rate=0.00008)\n",
    "    model.compile(loss= Integral_loss, optimizer=opt)\n",
    "    model.fit(Intervals, FInterval, batch_size = 32,  epochs = 75)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model for training larger dataset on the interval [low_bound, up_bound].\n",
    "\n",
    "def TrainModelArbitrary(Intervals, FInterval, x_val, y_val):\n",
    "    model = models.Sequential() \n",
    "    model.add(layers.Dense(128,  kernel_initializer='normal', activation = 'relu',  input_shape=(2, )))\n",
    "    model.add(layers.Dense(64,  kernel_initializer='normal', activation = 'relu', ))\n",
    "    model.add(Dense(1))\n",
    "    opt = optimizers.Adam(learning_rate=0.00008)\n",
    "    model.compile(loss= Integral_loss, optimizer=opt)\n",
    "    model.fit(Intervals, FInterval, batch_size = 32,  epochs = 30, validation_data = (x_val, y_val))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModelZeroRecurrent(Intervals, FInterval, x_val, y_val):\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(LSTM(64, input_shape=(1, 2)))\n",
    "    model.add(layers.Dense(64,  kernel_initializer='normal', activation = 'relu', ))\n",
    "    model.add(Dense(1))\n",
    "    opt = optimizers.Adam(learning_rate=0.00008)\n",
    "    model.compile(loss= Integral_loss, optimizer=opt)\n",
    "\n",
    "    model.fit(Intervals, FInterval, epochs=20, batch_size=32, validation_data = (x_val, y_val))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test on random intervals on the interval [low_bound, up_bound], \n",
    "#where x is always = low_bound and y is random on the interval \n",
    "#(low_bound + minWidth, up_bound).\n",
    "\n",
    "def TestModelZero(points, low_bound, up_bound, model, x_val, y_val):\n",
    "    import random\n",
    "    IntervalTest = np.zeros([1000,2])\n",
    "    FTest = np.zeros(1000)\n",
    "    relError = np.zeros(1000)\n",
    "    i = 0\n",
    "    min_width = (up_bound - low_bound) / points\n",
    "\n",
    "    while(i<1000):\n",
    "        x1 = low_bound\n",
    "        x2 = random.uniform(low_bound + 10*min_width,up_bound)\n",
    "    \n",
    "        IntervalTest[i,0] = x1\n",
    "        IntervalTest[i,1] = x2\n",
    "\n",
    "        FTest[i] = ( np.exp(x2) - np.exp(x1) )\n",
    "        i += 1\n",
    "\n",
    "    Predicted = model.predict(IntervalTest, verbose=0)\n",
    "    for i in range(0,1000):\n",
    "        relError[i] = abs(Predicted[i] - FTest[i]) / abs(FTest[i]) \n",
    "\n",
    "    return relError, IntervalTest, FTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test on random intervals of even length on the interval [low_bound,up_bound].\n",
    "def TestModelZeroPoint(points, low_bound, up_bound, model):\n",
    "    import random\n",
    "    IntervalTest = np.zeros([1000,1])\n",
    "    FTest = np.zeros(1000)\n",
    "    relError = np.zeros(1000)\n",
    "    i = 0\n",
    "    minWidth = (up_bound - low_bound) / points\n",
    "\n",
    "    while(i<1000):\n",
    "        \n",
    "        x = random.uniform(low_bound + 10*minWidth,up_bound)\n",
    "        IntervalTest[i] = x\n",
    "\n",
    "        FTest[i] = ( np.exp(x) - np.exp(0) )\n",
    "        i += 1\n",
    "\n",
    "    Predicted = model.predict(IntervalTest, verbose=0)\n",
    "    for i in range(0,1000):\n",
    "        relError[i] = abs(Predicted[i] - FTest[i]) / abs(FTest[i]) \n",
    "\n",
    "    return relError, IntervalTest, FTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on random intervals of fixed  length on the interval [low_bound, up_bound]\n",
    "def TestModelFixed(width, low_bound, up_bound, model):\n",
    "\n",
    "    IntervalTest = np.zeros([1000,2])\n",
    "    FTest = np.zeros(1000)\n",
    "    relError = np.zeros(1000)\n",
    "    i = 0\n",
    "   \n",
    "    while(i<1000):\n",
    "        x1 = random.uniform(low_bound, up_bound) #- 1.2*width) \n",
    "        x2 = x1 + width#*(1+random.uniform(-0.1, 0.1))\n",
    "    \n",
    "        IntervalTest[i,0] = x1\n",
    "        IntervalTest[i,1] = x2\n",
    "\n",
    "        FTest[i] = ( np.exp(IntervalTest[i,1]) - np.exp(IntervalTest[i,0]) )\n",
    "        i += 1\n",
    "    Predicted = model.predict(IntervalTest, verbose=0)\n",
    "    for i in range(0,1000):\n",
    "        relError[i] = abs(Predicted[i] - FTest[i]) / abs(FTest[i]) \n",
    "\n",
    "    return relError, IntervalTest, FTest\n",
    "\n",
    "#test on random intervals of fixed  length on the interval [low_bound, up_bound]\n",
    "def TestModelFixedPolynomial(width, low_bound, up_bound, power, model):\n",
    "\n",
    "    IntervalTest = np.zeros([1000,2])\n",
    "    FTest = np.zeros(1000)\n",
    "    relError = np.zeros(1000)\n",
    "    absError = np.zeros(1000)\n",
    "    i = 0\n",
    "   \n",
    "    while(i<1000):\n",
    "        x1 = random.uniform(low_bound, up_bound-width) #- 1.2*width) \n",
    "        x2 = x1 + width#*(1+random.uniform(-0.1, 0.1))\n",
    "    \n",
    "        IntervalTest[i,0] = x1\n",
    "        IntervalTest[i,1] = x2\n",
    "\n",
    "        FTest[i] = ( (IntervalTest[i,1])**(power+1) - (IntervalTest[i,0])**(power+1) )/(power+1) \n",
    "        i += 1\n",
    "    Predicted = model.predict(IntervalTest, verbose=0)\n",
    "    for i in range(0,1000):\n",
    "        relError[i] = abs(Predicted[i] - FTest[i]) /  abs(FTest[i])  \n",
    "        absError[i] =  abs(Predicted[i] - FTest[i])\n",
    "    return relError, IntervalTest, FTest, absError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on random intervals on the interval [low_bound, up_bound]\n",
    "def TestModelArbitrary(points, low_bound, up_bound, model):\n",
    "    import random\n",
    "    IntervalTest = np.zeros([1000,2])\n",
    "    FTest = np.zeros(1000)\n",
    "    relError = np.zeros(1000)\n",
    "    i = 0\n",
    "    minWidth = (up_bound - low_bound) / points\n",
    "\n",
    "    while(i<1000):\n",
    "        x11 = random.uniform(low_bound, up_bound - minWidth)\n",
    "        x12 = random.uniform(low_bound,up_bound)\n",
    "        x1 = min(x11,x12)\n",
    "        x2 = max(x11,x12)\n",
    "        if( (abs(x2-x1) < 10*minWidth)):\n",
    "            continue\n",
    "        IntervalTest[i,0] = x1\n",
    "        IntervalTest[i,1] = x2\n",
    "\n",
    "        FTest[i] = ( np.exp(x2) - np.exp(x1) )\n",
    "        i += 1\n",
    "\n",
    "    Predicted = model.predict(IntervalTest, verbose=0)\n",
    "    for i in range(0,1000):\n",
    "        relError[i] = abs(Predicted[i] - FTest[i]) / abs(FTest[i]) \n",
    "    \n",
    "    return relError, IntervalTest, FTest\n",
    "\n",
    "\n",
    "#test on random intervals of fixed length on the interval [low_bound, up_bound]\n",
    "def TestModelManyEvenly(points, low_bound, up_bound, model):\n",
    "    import random\n",
    "    IntervalTest = np.zeros([1000,2])\n",
    "    FTest = np.zeros(1000)\n",
    "    relError = np.zeros(1000)\n",
    "    i = 0\n",
    "    minWidth = (up_bound - low_bound) / (points - 1)\n",
    "\n",
    "    while(i<100):\n",
    "        x1 = random.uniform(low_bound, up_bound - 50*minWidth)\n",
    "        x2 = x1 + 50*minWidth\n",
    "      \n",
    "        IntervalTest[i,0] = x1\n",
    "        IntervalTest[i,1] = x2\n",
    "\n",
    "        FTest[i] = ( np.exp(x2) - np.exp(x1) )\n",
    "        i += 1\n",
    "\n",
    "    Predicted = model.predict(IntervalTest, verbose=0)\n",
    "    for i in range(0,1000):\n",
    "        relError[i] = abs(Predicted[i] - FTest[i]) / abs(FTest[i]) \n",
    "\n",
    "    return relError, IntervalTest, FTest\n",
    "\n",
    "#test on random intervals on the interval [low_bound, up_bound]\n",
    "def TestModelRecurrentRandom(points, low_bound, up_bound, model):\n",
    "    import random\n",
    "    IntervalTest = np.zeros([1000,2])\n",
    "    FTest = np.zeros(1000)\n",
    "    relError = np.zeros(1000)\n",
    "    i = 0\n",
    "    minWidth = (up_bound - low_bound) / points\n",
    "\n",
    "    while(i<1000):\n",
    "        x11 = random.uniform(low_bound, up_bound - minWidth)\n",
    "        x12 = random.uniform(low_bound,up_bound)\n",
    "        x1 = min(x11,x12)\n",
    "        x2 = max(x11,x12)\n",
    "        if( (abs(x2-x1) < 2*minWidth)):\n",
    "            continue\n",
    "        IntervalTest[i,0] = x1\n",
    "        IntervalTest[i,1] = x2\n",
    "\n",
    "        FTest[i] = ( np.exp(x2) - np.exp(x1) )\n",
    "        i += 1\n",
    "    \n",
    "    IntervalTestR = np.reshape(IntervalTest, (IntervalTest.shape[0], 1, IntervalTest.shape[1]))\n",
    "    Predicted = model.predict(IntervalTestR, verbose=0)\n",
    "    for i in range(0,1000):\n",
    "        relError[i] = abs(Predicted[i] - FTest[i]) / abs(FTest[i]) \n",
    "    \n",
    "    return relError, IntervalTest, FTest\n",
    "\n",
    "\n",
    "def TestModelRecurrentEven(width, low_bound, up_bound, model):\n",
    "\n",
    "    IntervalTest = np.zeros([1000,2])\n",
    "    FTest = np.zeros(1000)\n",
    "\n",
    "    relError = np.zeros(1000)\n",
    "    i = 0\n",
    "   \n",
    "    while(i<1000):\n",
    "        x1 = random.uniform(low_bound, up_bound - width) \n",
    "        x2 = x1 + width\n",
    "    \n",
    "        IntervalTest[i,0] = x1\n",
    "        IntervalTest[i,1] = x2\n",
    "\n",
    "        FTest[i] = ( np.exp(IntervalTest[i,1]) - np.exp(IntervalTest[i,0]) )\n",
    "        i += 1\n",
    "        \n",
    "    IntervalTest = np.reshape(IntervalTest, (IntervalTest.shape[0], 1, IntervalTest.shape[1]))\n",
    "    Predicted = model.predict(IntervalTest, verbose=0)\n",
    "    for i in range(0,1000):\n",
    "        relError[i] = abs(Predicted[i] - FTest[i]) / abs(FTest[i]) \n",
    "\n",
    "    return relError, IntervalTest, FTest\n",
    "\n",
    "#Test on random intervals on the interval [low_bound, up_bound], \n",
    "#where x is always = low_bound and y is random on the interval \n",
    "#(low_bound + minWidth, up_bound).\n",
    "\n",
    "def TestModelZeroRecurrent(points, low_bound, up_bound, model, x_val, y_val):\n",
    "    import random\n",
    "    IntervalTest = np.zeros([1000,2])\n",
    "    FTest = np.zeros(1000)\n",
    "    relError = np.zeros(1000)\n",
    "    i = 0\n",
    "    min_width = (up_bound - low_bound) / points\n",
    "\n",
    "    while(i<1000):\n",
    "        x1 = low_bound\n",
    "        x2 = random.uniform(low_bound + 10*min_width,up_bound)\n",
    "    \n",
    "        IntervalTest[i,0] = x1\n",
    "        IntervalTest[i,1] = x2\n",
    "\n",
    "        FTest[i] = ( np.exp(x2) - np.exp(x1) )\n",
    "        i += 1\n",
    "    IntervalTestR = np.reshape(IntervalTest, (IntervalTest.shape[0], 1, IntervalTest.shape[1]))\n",
    "    Predicted = model.predict(IntervalTestR, verbose=0)\n",
    "    for i in range(0,1000):\n",
    "        relError[i] = abs(Predicted[i] - FTest[i]) / abs(FTest[i]) \n",
    "\n",
    "    return relError, IntervalTest, FTest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValidationZero(numPoints, low_bound, up_bound):\n",
    "  \n",
    "    x_val  = np.zeros([1000, 2])\n",
    "    y_val = np.zeros(1000)\n",
    "    minWidth = (up_bound - low_bound)/ 1000\n",
    "    for i in range(1000):\n",
    "        x_val[i,0] = low_bound\n",
    "        x_val[i,1] = random.uniform(low_bound + minWidth, up_bound)\n",
    "    \n",
    "    for i in range(1000):\n",
    "        y_val[i] = np.exp(x_val[i,1]) - np.exp(x_val[i,0])\n",
    "    return x_val, y_val\n",
    "\n",
    "\n",
    "def ValidationFixed(width, low_bound, up_bound):\n",
    "    import random\n",
    "    x_val = np.zeros([1000,2])\n",
    "    y_val = np.zeros(1000)\n",
    "    relError = np.zeros(1000)\n",
    "    i = 0\n",
    "   \n",
    "    while(i<1000):\n",
    "        x1 = random.uniform(low_bound, up_bound - width) \n",
    "        x2 = x1 + width\n",
    "    \n",
    "        x_val[i,0] = x1\n",
    "        x_val[i,1] = x2\n",
    "\n",
    "        y_val[i] = (np.exp(x_val[i,1]) - np.exp(x_val[i,0]) )\n",
    "        i += 1\n",
    "    return x_val, y_val\n",
    "    \n",
    "def ValidationArbitrary(points, low_bound, up_bound):\n",
    "    import random\n",
    "    x_val = np.zeros([1000,2])\n",
    "    y_val = np.zeros(1000)\n",
    "    relError = np.zeros(1000)\n",
    "    min_width = (up_bound - low_bound)/points\n",
    "    i = 0\n",
    "   \n",
    "    while(i<1000):\n",
    "        x11 = random.uniform(low_bound, up_bound - min_width) \n",
    "        x12 = random.uniform(low_bound, up_bound)\n",
    "        x1 = min(x11,x12)\n",
    "        x2 = max(x11,x12)\n",
    "        if( (abs(x2-x1) < 2*minWidth)):\n",
    "            continue\n",
    "        x_val[i,0] = x1\n",
    "        x_val[i,1] = x2\n",
    "\n",
    "        y_val[i] = ( np.exp(x_val[i,1]) - np.exp(x_val[i,0]) )\n",
    "        i += 1\n",
    "    return x_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the test for the problem of a first kind: keep left endpoint always 0, whereas right endpoint ranges from [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 4.3774e-04 - val_loss: 3.7037e-04\n",
      "Epoch 2/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.9558e-04 - val_loss: 2.0666e-04\n",
      "Epoch 3/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.7926e-04 - val_loss: 1.6041e-04\n",
      "Epoch 4/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.4288e-04 - val_loss: 1.2223e-04\n",
      "Epoch 5/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 9.8117e-05 - val_loss: 7.0965e-05\n",
      "Epoch 6/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 4.6361e-05 - val_loss: 2.9230e-05\n",
      "Epoch 7/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.5479e-05 - val_loss: 2.1551e-05\n",
      "Epoch 8/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.8912e-05 - val_loss: 1.5565e-05\n",
      "Epoch 9/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3655e-05 - val_loss: 1.1016e-05\n",
      "Epoch 10/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 9.6733e-06 - val_loss: 7.8555e-06\n",
      "Epoch 11/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 7.0091e-06 - val_loss: 5.5336e-06\n",
      "Epoch 12/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 5.0862e-06 - val_loss: 3.9699e-06\n",
      "Epoch 13/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 3.7192e-06 - val_loss: 2.9743e-06\n",
      "Epoch 14/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.7061e-06 - val_loss: 2.1395e-06\n",
      "Epoch 15/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.0236e-06 - val_loss: 1.5807e-06\n",
      "Epoch 16/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.5243e-06 - val_loss: 1.3970e-06\n",
      "Epoch 17/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.1934e-06 - val_loss: 9.3572e-07\n",
      "Epoch 18/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 9.1158e-07 - val_loss: 7.4822e-07\n",
      "Epoch 19/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 7.2194e-07 - val_loss: 5.7237e-07\n",
      "Epoch 20/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 5.8877e-07 - val_loss: 5.0106e-07\n",
      "Epoch 21/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 4.8399e-07 - val_loss: 3.8115e-07\n",
      "Epoch 22/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 4.3233e-07 - val_loss: 3.0819e-07\n",
      "Epoch 23/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 3.6547e-07 - val_loss: 3.0516e-07\n",
      "Epoch 24/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 3.3095e-07 - val_loss: 2.4347e-07\n",
      "Epoch 25/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 3.0050e-07 - val_loss: 4.9674e-07\n",
      "Epoch 26/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.6623e-07 - val_loss: 2.0982e-07\n",
      "Epoch 27/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.5743e-07 - val_loss: 2.0084e-07\n",
      "Epoch 28/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.4816e-07 - val_loss: 2.0172e-07\n",
      "Epoch 29/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.4237e-07 - val_loss: 1.9632e-07\n",
      "Epoch 30/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3584e-07 - val_loss: 2.2467e-07\n",
      "Epoch 31/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.2248e-07 - val_loss: 1.8818e-07\n",
      "Epoch 32/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.1720e-07 - val_loss: 2.0293e-07\n",
      "Epoch 33/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0782e-07 - val_loss: 1.9733e-07\n",
      "Epoch 34/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.2275e-07 - val_loss: 2.1097e-07\n",
      "Epoch 35/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.1056e-07 - val_loss: 1.7877e-07\n",
      "Epoch 36/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0745e-07 - val_loss: 2.0058e-07\n",
      "Epoch 37/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.1033e-07 - val_loss: 2.7036e-07\n",
      "Epoch 38/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.1312e-07 - val_loss: 2.2454e-07\n",
      "Epoch 39/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0059e-07 - val_loss: 3.1437e-07\n",
      "Epoch 40/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.1102e-07 - val_loss: 1.8304e-07\n",
      "Epoch 41/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0476e-07 - val_loss: 1.9210e-07\n",
      "Epoch 42/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0222e-07 - val_loss: 2.3158e-07\n",
      "Epoch 43/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.9887e-07 - val_loss: 2.0385e-07\n",
      "Epoch 44/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0282e-07 - val_loss: 1.7257e-07\n",
      "Epoch 45/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0445e-07 - val_loss: 2.9946e-07\n",
      "Epoch 46/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0596e-07 - val_loss: 1.9662e-07\n",
      "Epoch 47/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.9278e-07 - val_loss: 2.4605e-07\n",
      "Epoch 48/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.9979e-07 - val_loss: 1.9503e-07\n",
      "Epoch 49/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.9900e-07 - val_loss: 1.8269e-07\n",
      "Epoch 50/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.9886e-07 - val_loss: 1.9663e-07\n",
      "Epoch 51/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0659e-07 - val_loss: 2.5223e-07\n",
      "Epoch 52/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0026e-07 - val_loss: 1.7235e-07\n",
      "Epoch 53/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0523e-07 - val_loss: 2.7229e-07\n",
      "Epoch 54/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.0618e-07 - val_loss: 2.6270e-07\n",
      "Epoch 55/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.9858e-07 - val_loss: 1.7298e-07\n",
      "Epoch 56/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0358e-07 - val_loss: 1.8236e-07\n",
      "Epoch 57/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.9638e-07 - val_loss: 2.3637e-07\n",
      "Epoch 58/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.9488e-07 - val_loss: 1.9933e-07\n",
      "Epoch 59/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0309e-07 - val_loss: 3.3320e-07\n",
      "Epoch 60/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0671e-07 - val_loss: 1.9221e-07\n",
      "Epoch 61/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.1220e-07 - val_loss: 2.2740e-07\n",
      "Epoch 62/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.9003e-07 - val_loss: 2.1008e-07\n",
      "Epoch 63/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0134e-07 - val_loss: 1.8432e-07\n",
      "Epoch 64/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0205e-07 - val_loss: 2.9871e-07\n",
      "Epoch 65/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0084e-07 - val_loss: 1.9122e-07\n",
      "Epoch 66/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.9923e-07 - val_loss: 1.7291e-07\n",
      "Epoch 67/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.1706e-07 - val_loss: 2.3739e-07\n",
      "Epoch 68/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.9544e-07 - val_loss: 2.0058e-07\n",
      "Epoch 69/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.9985e-07 - val_loss: 3.9748e-07\n",
      "Epoch 70/75\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.9504e-07 - val_loss: 1.8118e-07\n",
      "Epoch 71/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0174e-07 - val_loss: 1.9172e-07\n",
      "Epoch 72/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0059e-07 - val_loss: 1.7501e-07\n",
      "Epoch 73/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.9584e-07 - val_loss: 1.7692e-07\n",
      "Epoch 74/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.8937e-07 - val_loss: 1.7748e-07\n",
      "Epoch 75/75\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.9286e-07 - val_loss: 1.8102e-07\n",
      "2.071E-03\n",
      "1.563E-01\n",
      "9.025E-07\n",
      "training time = 54.97421014700012\n"
     ]
    }
   ],
   "source": [
    "#generate grid on an interval [0,1]\n",
    "import timeit\n",
    "startTime = timeit.default_timer()\n",
    "up_bound = 1.0\n",
    "low_bound = 0.0\n",
    "points = 10000\n",
    "width = (up_bound - low_bound)/5.0\n",
    "grid = np.linspace(low_bound, up_bound, points)\n",
    "Intervals, FInterval = IntervalsZero(points, grid)\n",
    "x_val, y_val = ValidationZero(points, low_bound, up_bound)\n",
    "model = TrainModelZero(Intervals, FInterval, x_val, y_val)\n",
    "\n",
    "relError, IntervalTest1, FTest = TestModelZero(points, low_bound, up_bound, model, x_val, y_val)\n",
    "relError = np.asarray(relError)\n",
    "for i in range(len(relError)):\n",
    "    relError[i] = '%.3E' % Decimal(relError[i])\n",
    "    \n",
    "print( '%.3E' % Decimal(relError.sum()/len(relError)))\n",
    "print(  '%.3E' % Decimal(max(relError)))\n",
    "print( '%.3E' % Decimal(min(relError)))\n",
    "modelTime = timeit.default_timer() - startTime\n",
    "print(\"training time =\", modelTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same type of problem tested on LSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 4.8743e-05 - val_loss: 1.0357e-05\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 3.4501e-06 - val_loss: 1.5089e-06\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 8.5328e-07 - val_loss: 3.6773e-07\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 2.5297e-07 - val_loss: 1.3757e-07\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2462e-07 - val_loss: 8.7410e-08\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 8.5898e-08 - val_loss: 1.1154e-07\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 7.3160e-08 - val_loss: 5.6623e-08\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 6.5450e-08 - val_loss: 5.3518e-08\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 6.2561e-08 - val_loss: 6.0281e-08\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 5.8563e-08 - val_loss: 8.0830e-08\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 5.7425e-08 - val_loss: 5.3081e-08\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 5.4876e-08 - val_loss: 4.6785e-08\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 5.4651e-08 - val_loss: 5.3780e-08\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 5.4968e-08 - val_loss: 4.4457e-08\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 5.2296e-08 - val_loss: 6.4662e-08\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 5.1817e-08 - val_loss: 4.9959e-08\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 5.1633e-08 - val_loss: 6.5593e-08\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 5.1131e-08 - val_loss: 4.4420e-08\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 4.9874e-08 - val_loss: 5.9480e-08\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 4.9712e-08 - val_loss: 4.0161e-08\n",
      "3.285E-03\n",
      "8.263E-01\n",
      "3.034E-07\n",
      "training time = 70.66926950200013\n"
     ]
    }
   ],
   "source": [
    "#generate grid on an interval [0,1]\n",
    "import timeit\n",
    "startTime = timeit.default_timer()\n",
    "up_bound = 1.0\n",
    "low_bound = 0.0\n",
    "points = 50000\n",
    "width = (up_bound - low_bound)/5.0\n",
    "grid = np.linspace(low_bound, up_bound, points)\n",
    "Intervals, FInterval = IntervalsZero(points, grid)\n",
    "IntervalsR = np.reshape(Intervals, (Intervals.shape[0], 1, Intervals.shape[1]))\n",
    "FIntervalR = np.reshape(FInterval, (FInterval.shape[0], 1, 1))\n",
    "\n",
    "x_val, y_val = ValidationZero(points, low_bound, up_bound)\n",
    "x_valR = np.reshape(x_val, (x_val.shape[0], 1, x_val.shape[1]))\n",
    "y_valR = np.reshape(y_val, (y_val.shape[0], 1, 1))\n",
    "model = TrainModelZeroRecurrent(IntervalsR, FIntervalR, x_valR, y_valR)\n",
    "\n",
    "relError, IntervalTest1, FTest = TestModelZeroRecurrent(points, low_bound, up_bound, model, x_val, y_val)\n",
    "relError = np.asarray(relError)\n",
    "for i in range(len(relError)):\n",
    "    relError[i] = '%.3E' % Decimal(relError[i])\n",
    "    \n",
    "print( '%.3E' % Decimal(relError.sum()/len(relError)))\n",
    "print(  '%.3E' % Decimal(max(relError)))\n",
    "print( '%.3E' % Decimal(min(relError)))\n",
    "modelTime = timeit.default_timer() - startTime\n",
    "print(\"training time =\", modelTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same problem but left endoint exluded from feature vector, since it is always 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "313/313 [==============================] - 0s 921us/step - loss: 4.3646e-04\n",
      "Epoch 2/75\n",
      "313/313 [==============================] - 0s 882us/step - loss: 3.4622e-04\n",
      "Epoch 3/75\n",
      "313/313 [==============================] - 0s 962us/step - loss: 2.5535e-04\n",
      "Epoch 4/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.9724e-04\n",
      "Epoch 5/75\n",
      "313/313 [==============================] - 0s 955us/step - loss: 1.7591e-04\n",
      "Epoch 6/75\n",
      "313/313 [==============================] - 0s 929us/step - loss: 1.6137e-04\n",
      "Epoch 7/75\n",
      "313/313 [==============================] - 0s 924us/step - loss: 1.4424e-04\n",
      "Epoch 8/75\n",
      "313/313 [==============================] - 0s 928us/step - loss: 1.2383e-04\n",
      "Epoch 9/75\n",
      "313/313 [==============================] - 0s 931us/step - loss: 1.0015e-04\n",
      "Epoch 10/75\n",
      "313/313 [==============================] - 0s 905us/step - loss: 7.4166e-05\n",
      "Epoch 11/75\n",
      "313/313 [==============================] - 0s 943us/step - loss: 5.0384e-05\n",
      "Epoch 12/75\n",
      "313/313 [==============================] - 0s 983us/step - loss: 3.6173e-05\n",
      "Epoch 13/75\n",
      "313/313 [==============================] - 0s 929us/step - loss: 3.1182e-05\n",
      "Epoch 14/75\n",
      "313/313 [==============================] - 0s 917us/step - loss: 2.9214e-05\n",
      "Epoch 15/75\n",
      "313/313 [==============================] - 0s 954us/step - loss: 2.7252e-05\n",
      "Epoch 16/75\n",
      "313/313 [==============================] - 0s 958us/step - loss: 2.4523e-05\n",
      "Epoch 17/75\n",
      "313/313 [==============================] - 0s 948us/step - loss: 2.1259e-05\n",
      "Epoch 18/75\n",
      "313/313 [==============================] - 0s 934us/step - loss: 1.7946e-05\n",
      "Epoch 19/75\n",
      "313/313 [==============================] - 0s 900us/step - loss: 1.4941e-05\n",
      "Epoch 20/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.2373e-05\n",
      "Epoch 21/75\n",
      "313/313 [==============================] - 0s 987us/step - loss: 1.0220e-05\n",
      "Epoch 22/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 8.4369e-06\n",
      "Epoch 23/75\n",
      "313/313 [==============================] - 0s 919us/step - loss: 7.0186e-06\n",
      "Epoch 24/75\n",
      "313/313 [==============================] - 0s 960us/step - loss: 5.8563e-06\n",
      "Epoch 25/75\n",
      "313/313 [==============================] - 0s 939us/step - loss: 4.9297e-06\n",
      "Epoch 26/75\n",
      "313/313 [==============================] - 0s 921us/step - loss: 4.1633e-06\n",
      "Epoch 27/75\n",
      "313/313 [==============================] - 0s 984us/step - loss: 3.5179e-06\n",
      "Epoch 28/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.0115e-06\n",
      "Epoch 29/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.5505e-06\n",
      "Epoch 30/75\n",
      "313/313 [==============================] - 0s 920us/step - loss: 2.2094e-06\n",
      "Epoch 31/75\n",
      "313/313 [==============================] - 0s 978us/step - loss: 1.9156e-06\n",
      "Epoch 32/75\n",
      "313/313 [==============================] - 0s 981us/step - loss: 1.6487e-06\n",
      "Epoch 33/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.4420e-06\n",
      "Epoch 34/75\n",
      "313/313 [==============================] - 0s 935us/step - loss: 1.2674e-06\n",
      "Epoch 35/75\n",
      "313/313 [==============================] - 0s 933us/step - loss: 1.1123e-06\n",
      "Epoch 36/75\n",
      "313/313 [==============================] - 0s 924us/step - loss: 1.0060e-06\n",
      "Epoch 37/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 9.0906e-07\n",
      "Epoch 38/75\n",
      "313/313 [==============================] - 0s 948us/step - loss: 8.1905e-07\n",
      "Epoch 39/75\n",
      "313/313 [==============================] - 0s 991us/step - loss: 7.4731e-07\n",
      "Epoch 40/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 7.0132e-07\n",
      "Epoch 41/75\n",
      "313/313 [==============================] - 0s 914us/step - loss: 6.4774e-07\n",
      "Epoch 42/75\n",
      "313/313 [==============================] - 0s 913us/step - loss: 6.0850e-07\n",
      "Epoch 43/75\n",
      "313/313 [==============================] - 0s 976us/step - loss: 5.6876e-07\n",
      "Epoch 44/75\n",
      "313/313 [==============================] - 0s 993us/step - loss: 5.5106e-07\n",
      "Epoch 45/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.2091e-07\n",
      "Epoch 46/75\n",
      "313/313 [==============================] - 0s 983us/step - loss: 5.1335e-07\n",
      "Epoch 47/75\n",
      "313/313 [==============================] - 0s 965us/step - loss: 4.9351e-07\n",
      "Epoch 48/75\n",
      "313/313 [==============================] - 0s 994us/step - loss: 4.8191e-07\n",
      "Epoch 49/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.7621e-07\n",
      "Epoch 50/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.6913e-07\n",
      "Epoch 51/75\n",
      "313/313 [==============================] - 0s 946us/step - loss: 4.5119e-07\n",
      "Epoch 52/75\n",
      "313/313 [==============================] - 0s 941us/step - loss: 4.4809e-07\n",
      "Epoch 53/75\n",
      "313/313 [==============================] - 0s 984us/step - loss: 4.4538e-07\n",
      "Epoch 54/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.4162e-07\n",
      "Epoch 55/75\n",
      "313/313 [==============================] - 0s 936us/step - loss: 4.4330e-07\n",
      "Epoch 56/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.3308e-07\n",
      "Epoch 57/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.3151e-07\n",
      "Epoch 58/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.2753e-07\n",
      "Epoch 59/75\n",
      "313/313 [==============================] - 0s 994us/step - loss: 4.2678e-07\n",
      "Epoch 60/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.2511e-07\n",
      "Epoch 61/75\n",
      "313/313 [==============================] - 0s 991us/step - loss: 4.2704e-07\n",
      "Epoch 62/75\n",
      "313/313 [==============================] - 0s 941us/step - loss: 4.2061e-07\n",
      "Epoch 63/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.1916e-07\n",
      "Epoch 64/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.1881e-07\n",
      "Epoch 65/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.1335e-07\n",
      "Epoch 66/75\n",
      "313/313 [==============================] - 0s 930us/step - loss: 4.1659e-07\n",
      "Epoch 67/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.1720e-07\n",
      "Epoch 68/75\n",
      "313/313 [==============================] - 0s 945us/step - loss: 4.1501e-07\n",
      "Epoch 69/75\n",
      "313/313 [==============================] - 0s 996us/step - loss: 4.1015e-07\n",
      "Epoch 70/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.0926e-07\n",
      "Epoch 71/75\n",
      "313/313 [==============================] - 0s 963us/step - loss: 4.1019e-07\n",
      "Epoch 72/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.1126e-07\n",
      "Epoch 73/75\n",
      "313/313 [==============================] - 0s 960us/step - loss: 4.0988e-07\n",
      "Epoch 74/75\n",
      "313/313 [==============================] - 0s 965us/step - loss: 4.0620e-07\n",
      "Epoch 75/75\n",
      "313/313 [==============================] - 0s 997us/step - loss: 4.0465e-07\n",
      "2.525E-03\n",
      "1.316E-01\n",
      "8.189E-08\n"
     ]
    }
   ],
   "source": [
    "up_bound = 1.0\n",
    "low_bound = 0.0\n",
    "points = 10000\n",
    "grid = np.linspace(low_bound, up_bound, num=points)\n",
    "\n",
    "Points, FInterval = IntervalsZeroPoint(points, grid)\n",
    "\n",
    "model = TrainModelZeroPoint(Points, FInterval)\n",
    "\n",
    "relError, IntervalTest, FTest = TestModelZeroPoint(points, low_bound, up_bound, model)\n",
    "\n",
    "relError = np.asarray(relError)\n",
    "for i in range(len(relError)):\n",
    "    relError[i] = '%.3E' % Decimal(relError[i])\n",
    "print( '%.3E' % Decimal(relError.sum()/len(relError)))\n",
    "print(  '%.3E' % Decimal(max(relError)))\n",
    "print( '%.3E' % Decimal(min(relError)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem of the second kind: Both left and right endpoints can be varied but the distance between them is a  constant width. In this example it is 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.5226e-05 - val_loss: 4.8827e-06\n",
      "Epoch 2/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.4537e-06 - val_loss: 2.8601e-06\n",
      "Epoch 3/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.2459e-06 - val_loss: 2.4783e-06\n",
      "Epoch 4/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.5715e-06 - val_loss: 1.7440e-06\n",
      "Epoch 5/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.6913e-06 - val_loss: 1.0945e-06\n",
      "Epoch 6/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 9.8048e-07 - val_loss: 4.0375e-07\n",
      "Epoch 7/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.6702e-07 - val_loss: 3.3554e-07\n",
      "Epoch 8/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.7356e-07 - val_loss: 2.9668e-07\n",
      "Epoch 9/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.7241e-07 - val_loss: 1.8700e-07\n",
      "Epoch 10/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.0989e-07 - val_loss: 1.6431e-07\n",
      "Epoch 11/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.7484e-07 - val_loss: 1.4695e-07\n",
      "Epoch 12/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.5227e-07 - val_loss: 1.2343e-07\n",
      "Epoch 13/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.3645e-07 - val_loss: 1.0975e-07\n",
      "Epoch 14/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.2458e-07 - val_loss: 9.5438e-08\n",
      "Epoch 15/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.1033e-07 - val_loss: 9.0829e-08\n",
      "Epoch 16/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.0254e-07 - val_loss: 7.6775e-08\n",
      "Epoch 17/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 9.8057e-08 - val_loss: 7.4202e-08\n",
      "Epoch 18/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 9.3686e-08 - val_loss: 1.0816e-07\n",
      "Epoch 19/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 9.7701e-08 - val_loss: 1.7064e-07\n",
      "Epoch 20/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 8.4860e-08 - val_loss: 6.3263e-08\n",
      "Epoch 21/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 8.0298e-08 - val_loss: 1.9501e-07\n",
      "Epoch 22/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 8.1594e-08 - val_loss: 8.2918e-08\n",
      "Epoch 23/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 7.7279e-08 - val_loss: 7.7714e-08\n",
      "Epoch 24/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 7.2751e-08 - val_loss: 6.2472e-08\n",
      "Epoch 25/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 7.4936e-08 - val_loss: 5.7751e-08\n",
      "Epoch 26/75\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 6.7046e-08 - val_loss: 6.2665e-08\n",
      "Epoch 27/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.7545e-08 - val_loss: 1.2253e-07\n",
      "Epoch 28/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.9098e-08 - val_loss: 6.2604e-08\n",
      "Epoch 29/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.6154e-08 - val_loss: 5.9219e-08\n",
      "Epoch 30/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.6710e-08 - val_loss: 6.1403e-08\n",
      "Epoch 31/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.1907e-08 - val_loss: 5.3602e-08\n",
      "Epoch 32/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.0476e-08 - val_loss: 5.3448e-08\n",
      "Epoch 33/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.4661e-08 - val_loss: 5.7528e-08\n",
      "Epoch 34/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.1621e-08 - val_loss: 7.5932e-08\n",
      "Epoch 35/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.7003e-08 - val_loss: 4.8671e-08\n",
      "Epoch 36/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.2557e-08 - val_loss: 4.9685e-08\n",
      "Epoch 37/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.7565e-08 - val_loss: 7.4171e-08\n",
      "Epoch 38/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.1084e-08 - val_loss: 5.2929e-08\n",
      "Epoch 39/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.7434e-08 - val_loss: 5.0471e-08\n",
      "Epoch 40/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.9190e-08 - val_loss: 1.0464e-07\n",
      "Epoch 41/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.9999e-08 - val_loss: 5.2381e-08\n",
      "Epoch 42/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.2308e-08 - val_loss: 4.6888e-08\n",
      "Epoch 43/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.6188e-08 - val_loss: 4.3181e-08\n",
      "Epoch 44/75\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 5.3885e-08 - val_loss: 4.6744e-08\n",
      "Epoch 45/75\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 4.9704e-08 - val_loss: 5.5892e-08\n",
      "Epoch 46/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.9088e-08 - val_loss: 4.2149e-08\n",
      "Epoch 47/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.4564e-08 - val_loss: 4.1089e-08\n",
      "Epoch 48/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.0594e-08 - val_loss: 4.1940e-08\n",
      "Epoch 49/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.2894e-08 - val_loss: 5.0853e-08\n",
      "Epoch 50/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.3985e-08 - val_loss: 4.2212e-08\n",
      "Epoch 51/75\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 5.0777e-08 - val_loss: 4.4852e-08\n",
      "Epoch 52/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.9726e-08 - val_loss: 4.2142e-08\n",
      "Epoch 53/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.7632e-08 - val_loss: 3.8246e-08\n",
      "Epoch 54/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.9064e-08 - val_loss: 4.6968e-08\n",
      "Epoch 55/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.6776e-08 - val_loss: 5.2196e-08\n",
      "Epoch 56/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.6192e-08 - val_loss: 5.4960e-08\n",
      "Epoch 57/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.4395e-08 - val_loss: 4.0981e-08\n",
      "Epoch 58/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.3872e-08 - val_loss: 3.9105e-08\n",
      "Epoch 59/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.6118e-08 - val_loss: 3.7589e-08\n",
      "Epoch 60/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.5202e-08 - val_loss: 3.5966e-08\n",
      "Epoch 61/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.5756e-08 - val_loss: 5.6352e-08\n",
      "Epoch 62/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.7649e-08 - val_loss: 4.4371e-08\n",
      "Epoch 63/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.9326e-08 - val_loss: 3.6561e-08\n",
      "Epoch 64/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.4183e-08 - val_loss: 8.1108e-08\n",
      "Epoch 65/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.7634e-08 - val_loss: 6.2211e-08\n",
      "Epoch 66/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.2402e-08 - val_loss: 4.0100e-08\n",
      "Epoch 67/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.4620e-08 - val_loss: 3.4443e-08\n",
      "Epoch 68/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.3621e-08 - val_loss: 3.4402e-08\n",
      "Epoch 69/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.6857e-08 - val_loss: 3.6758e-08\n",
      "Epoch 70/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.2608e-08 - val_loss: 3.7704e-08\n",
      "Epoch 71/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.1801e-08 - val_loss: 4.1057e-08\n",
      "Epoch 72/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.5320e-08 - val_loss: 4.7341e-08\n",
      "Epoch 73/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.3060e-08 - val_loss: 3.3111e-08\n",
      "Epoch 74/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.2875e-08 - val_loss: 3.4881e-08\n",
      "Epoch 75/75\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.3577e-08 - val_loss: 4.5531e-08\n",
      "3.726E-04\n",
      "1.633E-03\n",
      "3.986E-07\n",
      "model time = 33.014589776999856\n"
     ]
    }
   ],
   "source": [
    "startTime = timeit.default_timer()\n",
    "numIntervals = 10000\n",
    "width = (up_bound - low_bound)/10.0\n",
    "Intervals, FInterval = IntervalsRandomEven(numIntervals, width, low_bound, up_bound)\n",
    "x_val, y_val = ValidationFixed(width, low_bound, up_bound)\n",
    "model = TrainModelZero(Intervals, FInterval, x_val, y_val)\n",
    "\n",
    "relError, IntervalTest, FTest = TestModelFixed(width, low_bound, up_bound, model)\n",
    "relError = np.asarray(relError)\n",
    "for i in range(len(relError)):\n",
    "    relError[i] = '%.3E' % Decimal(relError[i])\n",
    "print( '%.3E' % Decimal(relError.sum()/len(relError)))\n",
    "print(  '%.3E' % Decimal(max(relError)))\n",
    "print( '%.3E' % Decimal(min(relError)))\n",
    "\n",
    "modelTime = timeit.default_timer() - startTime\n",
    "print(\"model time =\", modelTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next cell integrals of exponential function is tested on the interval it has not been trained on. \n",
    "If not too far, integrals are still approximated quite well, which means that model to some extent has learned \n",
    "behaviour of an integral of the exponential function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33740452]]\n",
      "0.3491797448826972\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros([1,2])\n",
    "a[0, 0] = 1.2\n",
    "a[0,1] = 1.3\n",
    "a_predict = model.predict(a)\n",
    "a_true  = np.exp(1.3) - np.exp(1.2)\n",
    "\n",
    "print(a_predict)\n",
    "print(a_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next few cells analyze integrals of polynomials, in the order x, x^2, x^3, x^4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 9.9632e-05\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 7.4719e-07\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.0221e-07\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.1361e-07\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.1547e-07\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.3053e-07\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.1127e-07\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.6038e-07\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.4047e-07\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.6736e-07\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.5274e-07\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3761e-07\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.6273e-07\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3344e-07\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.5698e-07\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.0835e-07\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3526e-07\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.4941e-07\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.6408e-07\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.7858e-07\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.7579e-07\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.0247e-07\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.5670e-07\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.9803e-07\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.1397e-07\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.0756e-07\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3419e-07\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.3396e-07\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.1960e-07\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.6838e-07\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.4425e-07\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.7430e-07\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.7293e-07\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3951e-07\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.9784e-07\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.2336e-07\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.5224e-07\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.6503e-07\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3699e-07\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.6982e-07\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3692e-07\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.5755e-07\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.7296e-07\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.9304e-07\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.8146e-07\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.4241e-07\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.7522e-07\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.5820e-07\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.2266e-07\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3854e-07\n",
      "1.466E-03\n",
      "4.933E-03\n",
      "8.954E-04\n",
      "model time = 19.462216314000216\n"
     ]
    }
   ],
   "source": [
    "startTime = timeit.default_timer()\n",
    "numIntervals = 10000\n",
    "power = 1\n",
    "up_bound = 4.0\n",
    "low_bound = 0.0\n",
    "width = 0.25\n",
    "Intervals, FInterval = IntervalsRandomEvenPolynomial(numIntervals, width, low_bound, up_bound, power)\n",
    "model = TrainModelPolynomial(Intervals, FInterval)\n",
    "\n",
    "relError, IntervalTest, FTest, absError = TestModelFixedPolynomial(width, low_bound, up_bound, power, model)\n",
    "relError = np.asarray(relError)\n",
    "for i in range(len(relError)):\n",
    "    relError[i] = '%.3E' % Decimal(relError[i])\n",
    "print( '%.3E' % Decimal(relError.sum()/len(relError)))\n",
    "print(  '%.3E' % Decimal(max(relError)))\n",
    "print( '%.3E' % Decimal(min(relError)))\n",
    "\n",
    "modelTime = timeit.default_timer() - startTime\n",
    "print(\"model time =\", modelTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 8.6472e-04\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.5334e-04\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.2005e-04\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.5656e-04\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.2690e-04\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 9.5759e-05\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.9432e-05\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.1583e-05\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.5910e-05\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3339e-05\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.5205e-05\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.0465e-05\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 7.4988e-06\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 5.7545e-06\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 4.7475e-06\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 3.7260e-06\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 3.2940e-06\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.7134e-06\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.5949e-06\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.0243e-06\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.9223e-06\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.7153e-06\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.5338e-06\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.3510e-06\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.2605e-06\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.1995e-06\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 1.1050e-06\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 1.1533e-06\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 9.7614e-07\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.0071e-06\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.0388e-06\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 8.8657e-07\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 9.6259e-07\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 8.0859e-07\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 8.3020e-07\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.0399e-06\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 7.7390e-07\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 7.4449e-07\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 8.1393e-07\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 7.6336e-07\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 7.4185e-07\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 7.3203e-07\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 8.1921e-07\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 7.7116e-07\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.6450e-07\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 8.2179e-07\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.8200e-07\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 7.1732e-07\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 7.9530e-07\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 6.7553e-07\n",
      "4.030E-03\n",
      "3.505E-01\n",
      "5.022E-07\n",
      "model time = 20.938205817999915\n"
     ]
    }
   ],
   "source": [
    "startTime = timeit.default_timer()\n",
    "numIntervals = 10000\n",
    "power = 2\n",
    "up_bound = 4.0\n",
    "low_bound = 0.0\n",
    "width = 0.25\n",
    "Intervals, FInterval = IntervalsRandomEvenPolynomial(numIntervals, width, low_bound, up_bound, power)\n",
    "model = TrainModelPolynomial(Intervals, FInterval)\n",
    "\n",
    "relError, IntervalTest, FTest, absError = TestModelFixedPolynomial(width, low_bound, up_bound, power, model)\n",
    "relError = np.asarray(relError)\n",
    "for i in range(len(relError)):\n",
    "    relError[i] = '%.3E' % Decimal(relError[i])\n",
    "print( '%.3E' % Decimal(relError.sum()/len(relError)))\n",
    "print(  '%.3E' % Decimal(max(relError)))\n",
    "print( '%.3E' % Decimal(min(relError)))\n",
    "\n",
    "modelTime = timeit.default_timer() - startTime\n",
    "print(\"model time =\", modelTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.369e-02 8.908e-04 3.661e-03 1.966e-03 1.276e-04 2.562e-04 2.691e-04\n",
      " 6.788e-04 1.861e-03 4.183e-04 3.091e-02 2.968e-01 7.780e-04 7.266e-05\n",
      " 8.603e-05 1.764e-04 3.668e-04 4.745e-04 1.426e-03 6.514e-04 1.055e-03\n",
      " 1.413e-04 4.233e-04 5.222e-04 3.156e-04 3.188e-04 2.645e-04 7.132e-05\n",
      " 2.653e-03 2.316e-03 1.432e-02 5.306e-02 2.426e-01 1.374e-04 4.659e-03\n",
      " 1.044e-03 3.431e-04 1.768e-03 7.660e-05 7.307e-05 4.854e-05 1.260e-03\n",
      " 1.074e-03 1.251e-03 3.530e-04 3.083e-04 1.426e-03 2.023e-04 2.893e-04\n",
      " 1.345e-03 3.494e-04 9.264e-04 4.591e-05 3.778e-05 1.604e-02 3.514e-06\n",
      " 8.839e-04 1.821e-05 2.714e-04 2.986e-04 1.912e-04 6.359e-04 1.458e-04\n",
      " 8.943e-04 1.174e-02 2.494e-04 1.128e-04 7.162e-04 6.995e-05 4.831e-05\n",
      " 4.923e-04 2.657e-02 5.098e-03 2.237e-04 1.259e-03 1.995e-03 8.847e-04\n",
      " 2.261e-04 3.231e-04 2.689e-04 9.385e-05 7.755e-04 6.749e-04 3.319e-04\n",
      " 4.043e-04 1.423e-04 1.877e-03 2.939e-03 1.664e-03 6.211e-04 4.942e-05\n",
      " 1.722e-04 2.657e-02 6.003e-04 4.654e-05 1.673e-04 3.466e-04 2.452e-04\n",
      " 1.410e-04 2.806e-04 3.495e-04 7.673e-05 4.563e-03 3.119e-04 1.219e-04\n",
      " 2.311e-03 1.593e-02 1.125e-03 3.326e-03 6.986e-05 9.792e-05 3.740e-04\n",
      " 7.687e-06 1.057e-03 2.100e-04 1.269e-04 8.749e-06 2.970e-04 1.209e-03\n",
      " 4.842e-05 2.375e-03 2.313e-04 1.572e-03 2.654e-04 3.784e-04 5.619e-04\n",
      " 3.159e-04 2.790e-04 2.103e-04 3.560e-05 1.896e-04 5.084e-04 5.712e-03\n",
      " 3.335e-05 2.514e-04 3.450e-04 3.526e-04 6.663e-03 9.729e-04 1.532e-02\n",
      " 4.605e-04 1.948e-03 3.107e-02 3.124e-04 1.653e-02 3.337e-04 7.740e-03\n",
      " 5.358e-04 1.547e-02 7.026e-05 3.231e-04 1.016e-04 1.257e-03 8.124e-05\n",
      " 3.197e-04 1.495e-04 2.292e-05 1.009e-03 1.730e-04 6.610e-04 9.385e-03\n",
      " 2.210e-05 3.797e-05 1.327e-04 5.487e-05 5.425e-06 8.143e-05 1.276e-04\n",
      " 2.570e-04 2.237e-03 1.202e-03 2.759e-04 9.111e-04 3.298e-04 3.493e-04\n",
      " 2.526e-04 6.931e-04 3.540e-04 6.987e-04 2.835e-04 7.448e-04 1.017e-03\n",
      " 1.373e-04 1.539e-04 6.233e-04 7.897e-05 1.727e-03 2.002e-05 5.345e-04\n",
      " 2.170e-04 1.885e-03 7.351e-04 4.626e-04 8.142e-04 2.950e-04 1.272e-03\n",
      " 5.903e-04 9.984e-03 9.519e-04 1.889e-05 5.960e-04 7.023e-04 5.504e-04\n",
      " 1.434e-04 3.855e-04 5.598e-05 1.379e-04 6.980e-04 2.864e-04 7.203e-04\n",
      " 2.977e-04 1.949e-03 5.217e-02 6.674e-05 5.806e-05 3.539e-04 2.760e-04\n",
      " 2.796e-04 9.809e-05 1.362e-03 3.342e-03 1.061e-03 2.967e-04 2.982e-05\n",
      " 4.591e-04 8.912e-05 1.263e-03 1.719e-03 2.859e-04 1.527e-04 8.366e-05\n",
      " 1.004e-02 2.103e-03 3.230e-05 3.741e-04 7.640e-04 2.265e-06 3.816e-05\n",
      " 1.964e-03 2.563e-04 7.271e-04 2.628e-02 4.920e-04 2.694e-06 5.291e-04\n",
      " 2.193e-05 6.899e-05 2.155e-04 1.192e-03 5.065e-02 3.916e-04 5.358e-04\n",
      " 4.714e-05 6.497e-05 2.025e-04 4.348e-04 4.761e-04 3.167e-04 3.606e-03\n",
      " 1.860e-03 3.410e-04 4.521e-04 1.706e-04 7.804e-04 1.085e-03 1.228e-04\n",
      " 4.786e-04 1.808e-03 2.318e-04 5.812e-05 1.582e-02 2.899e-03 3.341e-04\n",
      " 6.662e-05 7.270e-04 2.913e-04 3.004e-03 5.553e-04 3.881e-04 3.804e-04\n",
      " 2.485e-05 1.059e-03 1.953e-03 3.557e-04 2.387e-03 6.649e-04 5.674e-04\n",
      " 3.221e-04 1.272e-04 1.810e-04 7.115e-04 7.759e-04 6.340e-04 3.126e-04\n",
      " 1.923e-03 1.045e-02 3.333e-04 2.393e-04 2.059e-04 7.896e-03 1.972e-04\n",
      " 1.698e-04 3.279e-04 3.152e-03 9.719e-05 1.334e-03 4.801e-04 4.492e-04\n",
      " 7.103e-04 1.564e-03 3.506e-04 1.174e-02 1.453e-04 1.353e-04 2.933e-04\n",
      " 1.227e-04 1.172e-04 4.085e-03 2.072e-03 6.860e-03 1.541e-03 8.307e-04\n",
      " 2.919e-03 3.440e-04 1.454e-03 2.393e-04 9.695e-05 1.380e-04 1.811e-02\n",
      " 6.007e-04 1.738e-04 2.056e-03 4.481e-03 3.239e-04 5.833e-04 7.591e-04\n",
      " 1.029e-03 1.585e-03 3.866e-04 5.755e-04 3.618e-04 6.863e-04 4.179e-04\n",
      " 6.717e-04 2.495e-04 2.061e-04 4.269e-03 2.549e-04 1.048e-03 3.486e-04\n",
      " 2.821e-04 1.289e-04 1.247e-04 3.324e-04 1.463e-04 5.118e-02 6.864e-05\n",
      " 2.937e-04 7.028e-04 7.554e-04 5.577e-04 1.088e-01 1.653e-04 8.814e-05\n",
      " 3.356e-04 3.701e-04 3.892e-04 1.466e-02 7.920e-04 7.611e-04 8.697e-05\n",
      " 4.986e-05 2.490e-03 1.082e-04 3.574e-04 7.533e-04 8.959e-04 4.292e-02\n",
      " 4.868e-04 4.106e-04 5.625e-04 6.358e-04 6.737e-04 7.362e-04 1.832e-03\n",
      " 3.469e-04 9.676e-04 1.411e-04 5.592e-04 1.251e-03 1.032e-03 7.278e-04\n",
      " 7.233e-04 4.789e-04 3.527e-04 1.638e-04 1.244e-02 2.878e-04 1.021e-04\n",
      " 5.707e-04 4.147e-04 1.112e-04 3.907e-04 5.753e-05 9.788e-03 1.673e-04\n",
      " 8.630e-06 1.706e-03 5.377e-02 8.721e-03 8.222e-05 7.182e-05 1.590e-03\n",
      " 2.036e-04 1.374e-04 8.748e-03 3.159e-03 2.839e-04 1.136e-03 8.163e-05\n",
      " 2.838e-05 3.830e-04 1.390e-03 6.729e-04 5.042e-04 2.622e-04 4.043e-03\n",
      " 5.726e-04 2.124e-03 1.494e-04 1.004e-02 5.034e-02 1.492e-04 1.460e-03\n",
      " 1.799e-04 3.331e-03 6.446e-05 1.044e-03 8.249e-05 3.128e-04 6.804e-04\n",
      " 1.936e-03 1.904e-04 3.945e-04 3.222e-01 6.387e-04 5.892e-04 2.525e-04\n",
      " 2.798e-05 9.915e-03 3.010e-04 2.098e-03 5.019e-03 6.858e-04 1.726e-03\n",
      " 6.349e-05 6.741e-04 1.544e-03 1.486e-04 1.039e-03 5.780e-05 4.518e-04\n",
      " 9.665e-03 9.435e-03 1.683e-03 1.680e-03 7.902e-04 5.175e-02 1.386e-04\n",
      " 3.488e-04 1.785e-05 4.594e-04 1.722e-04 5.033e-02 1.656e-02 5.901e-05\n",
      " 7.052e-04 4.974e-05 1.241e-03 1.617e-04 2.886e-04 6.198e-04 8.918e-03\n",
      " 1.204e-03 1.871e-03 2.501e-03 2.268e-05 4.221e-03 1.069e-03 1.082e-02\n",
      " 3.089e-02 3.627e-03 2.485e-04 1.124e-04 8.447e-05 3.534e-04 2.010e-04\n",
      " 1.684e-03 1.082e-03 1.888e-04 2.307e-03 4.545e-05 2.295e-05 7.022e-05\n",
      " 1.214e-03 4.957e-04 5.913e-04 4.149e-04 1.416e-04 5.755e-05 1.044e-02\n",
      " 3.939e-04 6.448e-04 6.357e-04 9.125e-03 1.113e-03 1.069e-03 3.087e-04\n",
      " 3.549e-03 9.372e-04 1.606e-04 1.698e-04 3.276e-04 6.972e-06 1.434e-03\n",
      " 1.510e-03 7.579e-04 1.349e-04 5.053e-02 2.910e-04 1.249e-04 9.916e-03\n",
      " 6.393e-05 1.764e-04 3.089e-04 7.473e-04 7.659e-04 3.987e-06 1.559e-04\n",
      " 7.660e-05 4.499e-04 1.296e-03 5.478e-04 1.255e-03 4.352e-04 4.074e-05\n",
      " 4.855e-03 2.523e-04 4.648e-04 7.329e-04 6.162e-05 1.129e-04 6.861e-04\n",
      " 4.875e-05 7.143e-05 5.206e-05 5.140e-02 2.699e-04 5.022e-07 6.289e-04\n",
      " 3.688e-05 6.299e-05 2.761e-04 2.911e-03 1.040e-03 6.087e-04 1.494e-04\n",
      " 2.354e-03 1.981e-04 1.057e-03 2.707e-01 2.585e-05 2.285e-04 9.803e-04\n",
      " 2.285e-03 4.648e-04 4.865e-05 3.027e-04 2.302e-04 8.297e-03 4.637e-03\n",
      " 3.544e-04 1.895e-04 1.119e-04 1.084e-04 2.270e-03 2.261e-03 1.754e-04\n",
      " 7.323e-04 2.601e-04 1.073e-04 2.747e-04 5.034e-02 1.296e-03 1.493e-04\n",
      " 3.505e-01 4.056e-04 1.057e-04 4.341e-04 3.031e-04 2.097e-02 1.200e-03\n",
      " 2.146e-03 1.013e-04 1.671e-04 2.932e-04 1.396e-04 1.144e-03 2.577e-04\n",
      " 1.127e-02 2.878e-05 7.813e-04 1.656e-04 6.518e-03 3.427e-04 9.502e-04\n",
      " 2.531e-03 7.545e-04 3.208e-04 6.174e-04 6.596e-05 4.039e-04 1.499e-03\n",
      " 2.829e-04 2.754e-04 9.219e-04 4.110e-03 1.447e-02 5.797e-03 1.136e-04\n",
      " 1.233e-03 1.017e-05 2.392e-04 9.841e-04 1.019e-02 1.189e-04 7.901e-04\n",
      " 1.543e-04 1.331e-04 7.391e-03 3.825e-04 5.258e-05 8.769e-05 1.902e-03\n",
      " 1.279e-04 4.406e-05 1.634e-03 1.033e-04 9.804e-06 1.702e-03 2.003e-04\n",
      " 4.249e-03 3.635e-04 9.065e-04 4.465e-06 7.583e-04 3.794e-04 3.698e-04\n",
      " 3.889e-04 5.667e-04 8.597e-04 5.037e-02 1.720e-04 1.627e-06 1.597e-04\n",
      " 4.401e-03 8.096e-04 1.662e-04 8.337e-04 5.381e-06 7.079e-05 4.959e-04\n",
      " 1.939e-04 2.250e-04 1.246e-03 2.297e-05 4.672e-04 2.430e-04 4.042e-04\n",
      " 2.883e-04 2.259e-04 2.083e-04 6.887e-05 1.021e-02 6.090e-04 1.259e-04\n",
      " 3.434e-04 1.328e-03 1.464e-04 9.349e-04 8.793e-04 9.781e-05 3.445e-04\n",
      " 1.019e-02 2.952e-04 9.695e-04 5.119e-04 5.259e-04 5.160e-05 2.874e-04\n",
      " 4.672e-03 4.060e-04 2.467e-05 3.735e-04 1.316e-03 3.918e-04 9.399e-05\n",
      " 3.210e-04 2.401e-04 2.131e-03 1.295e-02 2.284e-02 1.989e-03 1.375e-05\n",
      " 4.919e-03 1.418e-04 4.878e-05 2.366e-05 1.571e-02 5.715e-03 7.622e-04\n",
      " 6.389e-04 1.036e-03 2.205e-04 2.216e-03 9.146e-05 5.532e-04 4.762e-05\n",
      " 3.225e-04 2.338e-03 1.453e-04 4.373e-04 4.984e-04 5.620e-05 9.523e-05\n",
      " 2.158e-04 1.886e-04 2.188e-04 6.407e-05 2.470e-04 5.664e-04 3.225e-04\n",
      " 2.745e-04 3.635e-04 9.551e-04 3.132e-05 1.948e-03 1.463e-04 1.751e-03\n",
      " 2.517e-04 1.513e-02 6.273e-04 4.943e-03 4.390e-05 6.465e-05 1.328e-04\n",
      " 1.018e-02 2.037e-03 1.128e-03 1.005e-02 1.153e-04 1.333e-03 4.570e-05\n",
      " 7.302e-04 2.379e-05 3.015e-04 1.999e-03 4.997e-04 3.030e-04 6.397e-04\n",
      " 1.017e-04 4.484e-04 7.449e-05 4.556e-03 4.738e-04 1.320e-03 2.118e-03\n",
      " 4.737e-04 3.234e-04 3.460e-03 8.217e-04 1.855e-04 2.435e-04 6.605e-05\n",
      " 5.163e-03 1.375e-04 2.107e-05 7.872e-04 1.830e-04 5.858e-04 1.308e-04\n",
      " 2.506e-03 5.438e-04 1.310e-04 3.784e-03 1.891e-04 3.127e-05 2.548e-03\n",
      " 1.552e-04 1.668e-04 1.140e-03 3.699e-02 8.803e-04 9.937e-04 8.002e-04\n",
      " 1.020e-03 1.344e-05 8.183e-05 2.763e-05 1.019e-03 1.654e-02 3.145e-04\n",
      " 8.167e-04 1.982e-03 1.768e-04 5.899e-05 1.538e-04 3.111e-05 4.097e-03\n",
      " 7.441e-04 3.721e-03 4.546e-05 3.536e-03 4.244e-04 8.587e-04 8.543e-03\n",
      " 1.430e-03 1.090e-05 3.175e-04 6.110e-04 2.348e-04 8.925e-03 9.997e-04\n",
      " 1.123e-03 2.004e-04 3.764e-04 6.728e-04 2.443e-03 3.494e-02 4.675e-03\n",
      " 2.900e-04 4.058e-05 2.605e-04 2.066e-03 1.729e-04 4.859e-04 1.643e-04\n",
      " 5.832e-05 1.109e-04 1.605e-05 5.127e-04 6.786e-05 4.661e-03 1.031e-04\n",
      " 3.395e-04 4.353e-02 8.581e-05 4.744e-02 3.483e-03 1.071e-03 7.860e-05\n",
      " 2.948e-04 8.040e-05 1.570e-04 8.173e-05 4.555e-04 1.537e-04 3.683e-04\n",
      " 6.305e-04 1.342e-04 3.815e-04 1.732e-04 1.573e-02 1.491e-02 2.702e-04\n",
      " 1.412e-03 1.200e-03 3.734e-03 6.862e-05 3.511e-04 1.850e-05 9.201e-05\n",
      " 1.002e-03 1.208e-04 6.053e-04 3.158e-04 7.699e-05 1.230e-04 2.590e-04\n",
      " 8.054e-04 7.510e-03 3.519e-04 5.785e-04 3.599e-04 4.190e-04 2.667e-04\n",
      " 8.757e-05 7.872e-04 1.644e-03 1.242e-02 3.474e-04 7.077e-04 2.134e-04\n",
      " 1.292e-02 1.425e-04 9.331e-05 1.749e-04 2.119e-03 2.213e-03 3.121e-04\n",
      " 1.649e-04 1.655e-02 9.155e-04 2.813e-04 7.007e-03 1.072e-03 3.518e-03\n",
      " 1.305e-02 4.822e-05 6.782e-04 1.506e-04 3.011e-04 2.987e-05 1.899e-04\n",
      " 3.225e-04 1.015e-02 4.243e-04 1.435e-04 4.194e-03 9.593e-04 1.602e-04\n",
      " 1.768e-04 4.376e-06 8.393e-05 1.807e-04 5.034e-04 1.568e-04 1.452e-04\n",
      " 5.574e-04 1.196e-04 2.312e-04 1.115e-03 1.388e-04 1.592e-03 4.659e-05\n",
      " 4.669e-05 1.562e-03 3.326e-04 1.032e-04 1.681e-04 1.691e-03 8.386e-05\n",
      " 9.981e-04 2.966e-04 1.304e-02 7.440e-04 5.616e-05 6.039e-04 5.732e-03\n",
      " 1.828e-03 3.412e-04 6.150e-05 2.955e-04 8.615e-04 3.527e-05 2.418e-03\n",
      " 2.234e-04 4.012e-04 1.039e-04 3.991e-04 4.557e-05 1.090e-04 9.200e-04\n",
      " 4.501e-03 5.030e-05 2.220e-03 4.349e-04 6.181e-05 3.221e-04 3.084e-04\n",
      " 2.866e-04 6.600e-04 1.756e-05 1.245e-04 1.018e-04 3.602e-04 7.138e-05\n",
      " 5.127e-04 2.821e-04 4.612e-05 3.818e-03 2.722e-04 2.921e-03 9.803e-04\n",
      " 6.892e-04 6.616e-04 1.666e-02 6.350e-05 8.520e-04 5.915e-04]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "axisX = (IntervalTest[:,1] + IntervalTest[:,0])/2\n",
    "axisY = relError\n",
    "\n",
    "plt.plot(axisX, axisY, 'o')\n",
    "print(relError)\n",
    "plt.savefig('pol2_plot.pdf')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.0021\n",
      "Epoch 2/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 9.6047e-04\n",
      "Epoch 3/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 5.6516e-04\n",
      "Epoch 4/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.9861e-04\n",
      "Epoch 5/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3554e-04\n",
      "Epoch 6/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 6.2475e-05\n",
      "Epoch 7/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 3.2956e-05\n",
      "Epoch 8/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.9598e-05\n",
      "Epoch 9/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3389e-05\n",
      "Epoch 10/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 9.6043e-06\n",
      "Epoch 11/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 7.3249e-06\n",
      "Epoch 12/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 5.8558e-06\n",
      "Epoch 13/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 4.8298e-06\n",
      "Epoch 14/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 4.2867e-06\n",
      "Epoch 15/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 3.7331e-06\n",
      "Epoch 16/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 3.3331e-06\n",
      "Epoch 17/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 3.2023e-06\n",
      "Epoch 18/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.9963e-06\n",
      "Epoch 19/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.8113e-06\n",
      "Epoch 20/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.7278e-06\n",
      "Epoch 21/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.6663e-06\n",
      "Epoch 22/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.6167e-06\n",
      "Epoch 23/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.5960e-06\n",
      "Epoch 24/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.6059e-06\n",
      "Epoch 25/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.4170e-06\n",
      "Epoch 26/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.4561e-06\n",
      "Epoch 27/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.4819e-06\n",
      "Epoch 28/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.5040e-06\n",
      "Epoch 29/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.4211e-06\n",
      "Epoch 30/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.4027e-06\n",
      "Epoch 31/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3081e-06\n",
      "Epoch 32/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.4494e-06\n",
      "Epoch 33/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3297e-06\n",
      "Epoch 34/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3306e-06\n",
      "Epoch 35/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3028e-06\n",
      "Epoch 36/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3925e-06\n",
      "Epoch 37/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3291e-06\n",
      "Epoch 38/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3558e-06\n",
      "Epoch 39/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3619e-06\n",
      "Epoch 40/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3462e-06\n",
      "Epoch 41/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.2733e-06\n",
      "Epoch 42/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.2916e-06\n",
      "Epoch 43/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.2540e-06\n",
      "Epoch 44/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3934e-06\n",
      "Epoch 45/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.2864e-06\n",
      "Epoch 46/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.2129e-06\n",
      "Epoch 47/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.1857e-06\n",
      "Epoch 48/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.1732e-06\n",
      "Epoch 49/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.2103e-06\n",
      "Epoch 50/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.2427e-06\n",
      "2.786E-02\n",
      "7.229E-01\n",
      "1.236E-05\n",
      "model time = 88.49889427899961\n"
     ]
    }
   ],
   "source": [
    "startTime = timeit.default_timer()\n",
    "numIntervals = 50000\n",
    "power = 3\n",
    "up_bound = 4.0\n",
    "low_bound = 0.0\n",
    "width = 0.25\n",
    "Intervals, FInterval = IntervalsRandomEvenPolynomial(numIntervals, width, low_bound, up_bound, power)\n",
    "model = TrainModelPolynomial(Intervals, FInterval)\n",
    "\n",
    "relError, IntervalTest, FTest, absError = TestModelFixedPolynomial(width, low_bound, up_bound, power, model)\n",
    "relError = np.asarray(relError)\n",
    "for i in range(len(relError)):\n",
    "    relError[i] = '%.3E' % Decimal(relError[i])\n",
    "print( '%.3E' % Decimal(relError.sum()/len(relError)))\n",
    "print(  '%.3E' % Decimal(max(relError)))\n",
    "print( '%.3E' % Decimal(min(relError)))\n",
    "\n",
    "modelTime = timeit.default_timer() - startTime\n",
    "print(\"model time =\", modelTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#some error analysis\n",
    "\n",
    "axisX = (IntervalTest[:,1] + IntervalTest[:,0])/2\n",
    "axisY = relError\n",
    "plt.figure(1)\n",
    "plt.plot(axisX, axisY, 'o')\n",
    "plt.savefig('pol3_plot.pdf')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(axisX, FTest, 'o')\n",
    "plt.savefig('F_plot.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.0099\n",
      "Epoch 2/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.0062\n",
      "Epoch 3/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.0048\n",
      "Epoch 4/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.0036\n",
      "Epoch 5/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.0026\n",
      "Epoch 6/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.0019\n",
      "Epoch 7/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.0014\n",
      "Epoch 8/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 9.7163e-04\n",
      "Epoch 9/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 6.5895e-04\n",
      "Epoch 10/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 4.5732e-04\n",
      "Epoch 11/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 3.3020e-04\n",
      "Epoch 12/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.4256e-04\n",
      "Epoch 13/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8512e-04\n",
      "Epoch 14/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.4306e-04\n",
      "Epoch 15/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.1489e-04\n",
      "Epoch 16/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 9.2817e-05\n",
      "Epoch 17/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 7.7714e-05\n",
      "Epoch 18/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 6.5159e-05\n",
      "Epoch 19/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 5.6212e-05\n",
      "Epoch 20/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 4.8167e-05\n",
      "Epoch 21/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 4.1070e-05\n",
      "Epoch 22/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 3.6625e-05\n",
      "Epoch 23/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 3.2142e-05\n",
      "Epoch 24/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.9307e-05\n",
      "Epoch 25/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.6184e-05\n",
      "Epoch 26/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.4337e-05\n",
      "Epoch 27/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.2401e-05\n",
      "Epoch 28/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.0284e-05\n",
      "Epoch 29/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8537e-05\n",
      "Epoch 30/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7818e-05\n",
      "Epoch 31/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7003e-05\n",
      "Epoch 32/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.5025e-05\n",
      "Epoch 33/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.4659e-05\n",
      "Epoch 34/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3768e-05\n",
      "Epoch 35/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3096e-05\n",
      "Epoch 36/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.2553e-05\n",
      "Epoch 37/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.2538e-05\n",
      "Epoch 38/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.1309e-05\n",
      "Epoch 39/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.1447e-05\n",
      "Epoch 40/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.0858e-05\n",
      "Epoch 41/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.0490e-05\n",
      "Epoch 42/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.0260e-05\n",
      "Epoch 43/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.0359e-05\n",
      "Epoch 44/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 9.8122e-06\n",
      "Epoch 45/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 9.8148e-06\n",
      "Epoch 46/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 9.4358e-06\n",
      "Epoch 47/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 8.7259e-06\n",
      "Epoch 48/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 8.7876e-06\n",
      "Epoch 49/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 9.2216e-06\n",
      "Epoch 50/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 8.9864e-06\n",
      "1.781E+00\n",
      "1.396E+02\n",
      "5.463E-05\n",
      "model time = 86.76772970499951\n"
     ]
    }
   ],
   "source": [
    "startTime = timeit.default_timer()\n",
    "numIntervals = 50000\n",
    "power = 4\n",
    "up_bound = 4.0\n",
    "low_bound = 0.0\n",
    "width = 0.25\n",
    "Intervals, FInterval = IntervalsRandomEvenPolynomial(numIntervals, width, low_bound, up_bound, power)\n",
    "model = TrainModelPolynomial(Intervals, FInterval)\n",
    "\n",
    "relError, IntervalTest, FTest, absError = TestModelFixedPolynomial(width, low_bound, up_bound, power, model)\n",
    "relError = np.asarray(relError)\n",
    "for i in range(len(relError)):\n",
    "    relError[i] = '%.3E' % Decimal(relError[i])\n",
    "print( '%.3E' % Decimal(relError.sum()/len(relError)))\n",
    "print(  '%.3E' % Decimal(max(relError)))\n",
    "print( '%.3E' % Decimal(min(relError)))\n",
    "\n",
    "modelTime = timeit.default_timer() - startTime\n",
    "print(\"model time =\", modelTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.784e-04 7.399e-04 1.243e-02 1.989e-03 1.726e-04 5.043e-04 6.177e-01\n",
      " 1.764e-01 5.048e-04 3.580e-04 1.735e-03 9.951e-03 2.044e+00 3.841e-04\n",
      " 9.401e-03 6.495e-04 3.812e-03 4.281e-04 3.473e+01 1.554e-02 1.172e-02\n",
      " 7.904e-02 6.546e-04 1.002e-03 2.738e-03 3.596e-02 1.196e-02 5.002e-03\n",
      " 4.264e-04 1.106e-02 2.898e-01 2.409e-02 4.980e-04 4.866e+00 4.010e-04\n",
      " 3.926e-04 8.811e-02 9.633e-03 3.509e-04 1.200e-03 2.126e-03 2.819e-04\n",
      " 2.391e-04 2.709e-03 3.240e-03 7.649e-03 2.930e-03 4.058e-04 3.795e-04\n",
      " 1.121e-03 8.940e-04 1.368e-02 9.798e-04 2.427e-02 1.218e-03 1.396e+02\n",
      " 8.512e-05 1.436e-03 1.677e-01 1.244e+02 1.320e-01 3.704e-02 9.106e-03\n",
      " 4.391e-03 8.530e-04 1.278e-01 5.678e-03 8.989e-04 3.699e-02 2.778e-04\n",
      " 1.104e-03 5.519e-04 2.401e-02 1.035e-03 3.356e-02 1.640e-03 1.071e-03\n",
      " 1.621e-04 3.515e-04 4.133e-04 4.000e-04 9.508e-03 1.343e-03 1.116e-03\n",
      " 4.514e-04 3.523e-02 5.412e-04 1.881e-03 1.475e-02 3.834e-02 1.237e-01\n",
      " 9.388e-04 9.926e-03 1.790e-03 1.915e-01 9.685e-04 5.268e+00 3.027e-02\n",
      " 1.928e-03 1.188e-02 6.203e+00 1.052e-02 7.424e+00 5.638e-03 1.094e-03\n",
      " 6.734e-04 9.922e-03 1.424e-03 1.717e-03 2.923e-03 4.487e-04 1.746e-03\n",
      " 6.378e-03 7.771e-04 3.113e-03 9.383e-03 6.376e-04 4.324e-01 7.078e-04\n",
      " 3.315e-04 8.776e-04 9.609e-04 4.451e-04 3.082e-02 7.154e-04 1.917e-03\n",
      " 4.618e-02 5.735e-02 7.948e-04 3.968e-04 3.024e-02 3.660e-04 2.921e-03\n",
      " 4.836e-04 2.439e-02 5.524e-05 4.921e-04 1.218e-03 3.586e-04 2.824e-03\n",
      " 1.659e-03 4.359e-04 3.948e-03 7.464e-04 2.131e-03 3.290e-04 1.068e-02\n",
      " 8.681e-04 4.962e-04 5.466e+01 5.288e-04 8.030e-03 2.369e-02 2.842e-01\n",
      " 4.283e-03 9.111e-03 9.456e-03 3.555e-02 7.684e-04 1.354e-03 3.594e-02\n",
      " 5.824e-04 3.450e-03 1.191e-03 2.272e-03 3.723e-04 4.027e-04 3.063e-04\n",
      " 1.267e-04 8.235e-04 4.837e-04 3.797e-02 4.215e-04 4.171e-03 4.385e-02\n",
      " 2.539e-04 2.353e-02 2.841e-02 1.380e-02 3.011e-04 2.043e-01 1.033e-02\n",
      " 4.189e-03 1.056e-03 2.406e-02 2.763e-03 3.892e-03 9.534e-03 2.204e+01\n",
      " 7.323e-04 7.935e-04 1.299e-02 1.462e+00 1.021e-03 4.833e-02 3.018e-04\n",
      " 3.121e-02 2.633e-01 6.650e-04 1.639e-01 4.129e-03 9.065e-04 3.401e-04\n",
      " 2.456e-03 9.054e-02 5.857e-04 1.092e-02 1.429e-02 2.879e-04 7.346e-04\n",
      " 2.412e-02 3.201e-04 1.059e-03 3.831e-03 1.365e-03 4.007e-04 7.590e-02\n",
      " 3.148e-03 1.760e-04 2.853e-02 6.020e-05 1.068e-03 2.521e-03 2.547e-04\n",
      " 7.596e-04 2.382e-01 1.383e-02 3.655e+00 4.149e-04 2.209e+00 1.294e-03\n",
      " 1.211e-02 1.190e-02 1.075e-02 5.097e-02 4.958e-04 3.791e-03 5.464e-04\n",
      " 4.684e-03 2.472e-02 7.548e-03 1.826e-03 3.526e-04 7.306e-04 2.892e-04\n",
      " 2.377e-02 2.437e-03 7.085e-03 2.539e-04 2.240e-03 3.273e-02 1.750e-03\n",
      " 8.664e-03 2.139e-02 3.773e-03 3.183e-03 4.656e-04 1.380e-03 1.201e-02\n",
      " 3.781e-04 8.784e-03 5.252e-03 1.050e-02 3.845e+01 4.033e-03 3.000e+01\n",
      " 4.944e-04 4.010e-04 3.267e-02 1.784e-01 1.106e+01 2.973e-01 5.815e-04\n",
      " 1.152e-01 9.000e-04 2.702e-02 2.056e-03 1.502e-03 2.096e-03 9.706e-03\n",
      " 5.903e-04 2.882e-04 7.901e-04 8.096e-04 2.847e-04 1.053e-02 1.383e-02\n",
      " 1.294e-04 9.899e-03 1.157e-02 1.133e-02 1.512e-01 1.337e-01 7.098e-04\n",
      " 1.172e-04 3.972e-04 5.480e-04 2.236e-03 1.078e-02 1.652e-01 3.220e-01\n",
      " 1.364e-01 6.533e-04 4.981e-03 3.437e-02 4.218e-03 5.211e+00 7.228e-03\n",
      " 1.265e-04 1.579e-02 3.066e-04 5.592e-02 3.768e-04 1.224e-03 4.970e-04\n",
      " 3.133e-03 9.533e-04 1.389e+00 5.362e-04 4.660e-04 4.536e-04 1.143e-02\n",
      " 8.305e-04 3.019e-02 1.806e-01 6.964e-04 3.957e-04 3.275e+01 1.289e-03\n",
      " 4.308e-03 3.012e-03 1.678e-03 1.142e-02 6.899e-04 7.486e-01 1.626e-03\n",
      " 2.485e-04 2.741e-04 1.403e-03 4.654e-03 1.516e+00 3.178e-03 5.147e-04\n",
      " 4.374e-03 1.269e-04 1.104e-02 1.119e-03 7.212e-03 1.858e-03 5.094e-04\n",
      " 5.874e-04 6.387e-04 3.548e-04 2.636e-02 3.690e-04 1.585e-01 3.005e-03\n",
      " 5.364e-04 1.993e-03 1.553e-03 1.240e-03 1.371e-02 4.853e-03 1.871e-01\n",
      " 4.019e-03 2.354e-02 1.409e-01 1.348e-02 2.537e-04 4.304e-04 3.650e+01\n",
      " 1.028e-03 7.903e-04 1.183e-03 1.408e-01 1.922e-04 2.821e-04 8.467e-04\n",
      " 2.678e-02 3.476e-04 2.763e-04 5.689e-02 2.357e-03 9.315e-04 3.568e-03\n",
      " 4.077e-03 3.620e-04 5.532e-04 2.406e-02 2.845e-03 3.450e-04 3.683e-04\n",
      " 4.705e-03 1.727e-03 3.262e-02 3.013e-02 4.351e-04 1.716e-03 2.822e-02\n",
      " 1.599e-02 2.134e-03 8.626e-04 3.893e-02 1.011e-04 3.903e-04 3.095e-02\n",
      " 3.663e-03 2.567e-02 3.884e-04 1.353e-02 2.195e-04 9.402e+01 1.215e-03\n",
      " 6.244e-04 1.231e-02 5.093e-03 8.757e-03 1.832e-01 5.190e-04 1.144e-02\n",
      " 2.034e+00 4.685e-04 9.849e-03 7.514e+01 1.111e-04 2.875e-03 3.981e+00\n",
      " 3.266e-02 1.119e-01 1.074e-01 5.017e-04 1.371e-02 1.338e-03 4.624e-02\n",
      " 7.748e-05 1.214e-03 2.438e-02 3.697e-04 1.113e-03 4.835e-02 3.188e-03\n",
      " 2.744e-03 1.003e-03 1.084e-03 1.041e-03 2.664e-04 8.757e-03 9.019e-04\n",
      " 2.938e-03 2.871e-02 2.553e-04 1.003e-01 3.392e-02 3.199e-02 3.020e-03\n",
      " 2.890e-03 3.319e-02 4.154e-04 9.362e-03 2.575e-04 4.240e-03 1.200e-03\n",
      " 2.364e-01 7.243e-04 5.512e-04 8.239e-04 1.003e-02 4.201e-02 3.963e-02\n",
      " 1.817e-01 1.272e-02 1.344e-01 1.187e-03 5.975e-04 4.465e-04 3.773e-04\n",
      " 1.198e-02 6.789e+01 7.070e-03 3.698e-02 9.932e-04 1.116e-03 1.415e-03\n",
      " 3.624e-02 1.065e-02 4.093e-04 5.483e-04 1.249e-01 2.060e+00 3.993e-04\n",
      " 2.356e-03 5.815e-04 3.974e-02 3.526e-04 2.163e-02 2.383e-04 1.461e-02\n",
      " 7.156e-04 4.293e+01 1.580e-01 2.839e-04 4.154e-04 3.411e-03 4.964e-03\n",
      " 1.509e-03 5.049e-04 1.138e-03 1.579e-03 1.334e-03 5.463e-05 3.064e-02\n",
      " 1.784e-01 5.194e-04 5.933e-04 4.671e-04 3.091e-03 1.927e-01 3.975e-04\n",
      " 4.019e-03 1.387e-03 3.483e-04 3.372e-02 5.988e-04 3.633e-04 1.387e-02\n",
      " 5.915e-04 3.414e-04 1.000e-02 3.074e+00 4.217e-04 1.854e-03 4.883e-04\n",
      " 2.379e-04 4.069e+00 4.139e-03 6.186e-04 1.517e-03 9.615e-03 3.634e-04\n",
      " 1.384e-02 1.165e-02 7.074e-02 1.835e-03 1.356e-03 4.325e-03 2.110e-04\n",
      " 9.546e-04 2.407e-02 1.321e-03 5.095e-02 4.297e-04 5.447e+00 2.324e-04\n",
      " 4.378e-04 4.667e-04 3.796e-04 5.544e-04 3.557e-03 5.828e-01 1.646e-03\n",
      " 1.337e-03 2.582e-03 8.254e-04 2.104e-03 8.303e-02 7.269e-03 4.755e-04\n",
      " 9.300e-04 1.470e-02 5.958e-04 8.655e-03 4.059e-04 5.470e-04 2.875e-04\n",
      " 1.486e+00 5.612e-03 6.153e-03 2.421e-02 1.464e-02 5.333e-04 3.273e-04\n",
      " 7.783e-04 5.853e-02 3.976e-03 4.681e-02 3.500e-03 3.168e-02 1.878e-03\n",
      " 9.422e-03 1.434e+01 7.926e-04 1.027e-03 1.429e-01 2.172e-02 9.823e-02\n",
      " 8.948e-04 4.848e-04 5.166e-03 7.555e-02 3.070e-04 8.496e-04 8.634e-04\n",
      " 1.340e-02 1.021e-03 2.216e-04 4.530e-04 3.119e-03 3.319e-02 3.067e-03\n",
      " 1.290e-04 2.250e-01 3.133e-04 1.122e+02 7.891e-04 8.947e+01 4.243e-04\n",
      " 1.380e-02 3.346e-04 7.356e-02 2.859e-03 2.326e-03 5.322e-04 1.137e-03\n",
      " 2.630e-04 1.213e-03 2.046e-01 8.080e-04 4.660e-02 9.013e-03 8.971e-03\n",
      " 1.393e-02 1.478e-01 3.169e-02 1.313e-01 1.424e-02 5.402e-02 3.259e-02\n",
      " 2.370e+00 4.251e-03 3.687e-04 2.550e-01 1.307e-02 9.190e-04 5.891e+01\n",
      " 7.712e-03 9.055e-04 1.888e-04 8.698e-03 3.455e-02 6.663e-04 1.770e-02\n",
      " 3.881e-04 1.436e-01 3.420e-04 3.939e-04 4.001e-04 8.550e-02 2.164e-03\n",
      " 3.874e-03 1.053e-02 3.807e-03 2.835e-04 8.534e-04 8.787e-02 1.107e-03\n",
      " 3.339e-04 5.083e-03 1.306e-01 2.339e-04 6.777e-04 2.558e-04 1.211e-03\n",
      " 1.817e-01 4.516e-04 1.311e-03 9.504e+01 5.329e-04 3.516e-02 9.168e-03\n",
      " 9.819e-03 1.656e+01 3.791e-04 4.253e-04 3.232e-03 6.197e-04 3.573e-04\n",
      " 4.672e-04 4.600e-04 2.628e-03 2.762e-01 8.100e-03 1.586e-02 1.204e-04\n",
      " 1.189e+01 1.582e-01 2.139e-03 2.365e-02 2.880e-01 1.904e-03 2.527e-01\n",
      " 1.218e-02 5.931e-04 7.965e-05 3.519e-04 2.077e-03 3.421e+00 8.685e-04\n",
      " 4.529e-04 3.865e-04 4.406e-02 5.100e-04 8.397e-03 2.736e+00 1.044e-03\n",
      " 4.819e-04 1.035e-03 4.023e-02 1.276e-03 1.466e-02 1.409e-03 7.777e-04\n",
      " 1.237e-02 2.801e-04 5.465e-04 3.165e-03 9.402e-04 3.678e-03 6.703e-02\n",
      " 4.867e-04 4.047e-04 6.984e-04 2.905e-04 6.484e-02 3.609e-02 3.848e-04\n",
      " 5.138e-03 8.846e-03 4.818e-04 9.884e-03 6.934e-04 4.769e-04 3.137e-04\n",
      " 4.259e-04 9.856e-03 2.868e-03 2.067e-03 2.338e-03 3.326e-03 4.334e-03\n",
      " 4.265e-03 3.635e-04 1.012e-03 3.684e-03 3.506e-03 5.182e-03 8.245e-04\n",
      " 3.362e-03 3.904e-04 9.858e-03 2.638e-03 4.956e-04 2.939e+01 4.461e-04\n",
      " 1.584e-03 2.127e-03 1.556e-01 2.545e-04 7.591e+00 1.486e-02 3.879e-03\n",
      " 6.654e-03 1.147e-02 1.449e-03 3.291e-04 1.437e-03 2.469e-04 2.210e-01\n",
      " 2.168e-01 4.120e-03 2.288e-04 2.552e-01 3.437e-04 3.446e-02 1.210e-03\n",
      " 3.171e-03 4.870e-02 1.079e-02 1.022e-02 9.432e-03 2.871e-04 1.319e-03\n",
      " 2.898e-04 9.094e-04 1.221e-02 2.013e-03 4.748e-04 3.653e-04 1.380e-02\n",
      " 5.044e-04 1.217e-02 9.360e-03 1.049e-02 4.856e-03 1.788e+01 1.606e-02\n",
      " 2.706e-03 5.313e-04 3.945e-04 4.308e-04 3.942e-04 9.027e-02 1.546e-03\n",
      " 7.368e-04 3.261e-02 4.398e-03 3.493e-03 8.060e-03 5.522e-04 2.457e-02\n",
      " 6.282e-04 1.093e-02 5.268e-03 5.735e-03 2.440e-01 1.124e-02 5.085e-04\n",
      " 1.846e-03 1.410e-02 8.466e-04 3.502e-04 3.910e-03 2.736e-02 1.059e-03\n",
      " 3.547e-02 2.421e-02 5.170e-04 4.079e-03 1.042e-02 3.208e-03 1.698e-01\n",
      " 5.533e-04 5.470e-03 1.187e-01 4.415e-02 6.137e-04 5.884e-04 9.809e-02\n",
      " 2.385e-02 5.782e+00 8.252e-04 3.090e-04 1.071e-02 7.455e-04 3.164e-03\n",
      " 4.769e-02 1.307e+02 6.570e-01 3.418e-04 9.809e-02 1.386e-03 1.802e-03\n",
      " 9.339e-03 3.589e-04 2.753e-01 7.411e-02 1.375e-02 5.784e-04 8.006e-04\n",
      " 7.053e-01 2.427e-03 5.281e-04 3.597e-03 1.392e-02 5.485e-04 6.359e+00\n",
      " 1.102e-02 1.294e-03 2.845e-04 2.398e-02 3.634e-04 2.688e-04 3.661e-04\n",
      " 2.991e-04 2.349e-04 1.190e-01 1.248e-03 2.704e-03 2.368e-01 7.270e-02\n",
      " 1.177e+02 7.423e-02 6.151e-04 4.105e-04 5.026e-04 3.307e-02 3.101e-03\n",
      " 1.319e-04 1.093e-02 3.497e-03 1.183e+00 1.256e-02 3.345e-02 6.649e-03\n",
      " 8.844e-04 2.718e-04 2.366e-02 1.806e-01 2.847e-01 2.427e-03 3.038e-02\n",
      " 5.265e-04 2.113e-02 5.850e-04 2.120e+01 4.387e-04 2.494e-02 1.713e-01\n",
      " 1.318e-01 6.429e-04 4.528e-04 2.219e-04 7.557e+00 3.809e-04 2.222e-01\n",
      " 1.525e-03 5.974e-02 9.820e-04 3.702e-04 8.978e-04 1.974e-01 3.722e+00\n",
      " 5.252e-02 1.368e+00 1.511e-02 3.848e-04 2.365e-02 2.214e-04 4.442e+01\n",
      " 1.567e-01 2.909e-04 9.101e-04 5.269e-04 3.684e-04 8.766e-03 6.832e-04\n",
      " 2.258e-04 2.816e-02 2.918e-02 3.279e-03 1.317e-01 3.922e-03 2.023e-03\n",
      " 4.953e+01 3.169e-02 4.471e-03 2.433e-03 1.673e-01 4.779e-04 2.938e+01\n",
      " 5.547e-02 3.496e-02 8.259e-04 5.165e-02 1.231e-02 1.904e-03 3.803e-02\n",
      " 7.827e-03 2.393e-02 1.464e-02 1.282e-02 1.982e+00 1.096e-03 3.182e-02\n",
      " 1.241e-03 6.670e+00 3.307e-02 1.152e-03 8.295e-03 3.607e-04 2.863e-04\n",
      " 6.539e-04 4.341e-04 4.820e-04 9.420e-03 1.383e-02 1.154e-02 2.442e-04\n",
      " 1.383e-02 1.015e-02 9.832e-04 1.121e-02 4.212e-03 1.126e-02 3.452e-04\n",
      " 9.978e-03 7.988e-03 3.855e-04 3.789e-04 3.860e-03 8.699e-03]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "axisX = ((IntervalTest[:,0]) + (IntervalTest[:,1]))/2\n",
    "axisY = relError\n",
    "\n",
    "plt.plot(axisX, axisY, 'o')\n",
    "print(relError)\n",
    "plt.savefig('pol4_plot.pdf')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third kind of problem: Both length of an interval and both endpoints are aribtary\n",
    "on interval [0,1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 0.0066 - val_loss: 4.3809e-04\n",
      "Epoch 2/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 9.6570e-04 - val_loss: 3.3543e-04\n",
      "Epoch 3/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 5.0818e-04 - val_loss: 2.0574e-04\n",
      "Epoch 4/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 2.0651e-04 - val_loss: 9.1410e-05\n",
      "Epoch 5/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 1.0906e-04 - val_loss: 8.7642e-05\n",
      "Epoch 6/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 7.2026e-05 - val_loss: 5.2268e-05\n",
      "Epoch 7/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 5.6111e-05 - val_loss: 4.1762e-05\n",
      "Epoch 8/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 4.8219e-05 - val_loss: 4.0365e-05\n",
      "Epoch 9/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 4.3126e-05 - val_loss: 5.8965e-05\n",
      "Epoch 10/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 3.9896e-05 - val_loss: 4.2057e-05\n",
      "Epoch 11/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 3.8378e-05 - val_loss: 3.2822e-05\n",
      "Epoch 12/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 3.7390e-05 - val_loss: 2.5501e-05\n",
      "Epoch 13/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 3.6438e-05 - val_loss: 4.9739e-05\n",
      "Epoch 14/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 3.4652e-05 - val_loss: 2.8010e-05\n",
      "Epoch 15/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 3.4005e-05 - val_loss: 3.0553e-05\n",
      "Epoch 16/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 3.4070e-05 - val_loss: 3.2563e-05\n",
      "Epoch 17/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 3.3215e-05 - val_loss: 2.6418e-05\n",
      "Epoch 18/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 3.2815e-05 - val_loss: 2.6669e-05\n",
      "Epoch 19/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 3.1899e-05 - val_loss: 2.7519e-05\n",
      "Epoch 20/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 3.1909e-05 - val_loss: 3.5621e-05\n",
      "Epoch 21/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 3.0944e-05 - val_loss: 2.9694e-05\n",
      "Epoch 22/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 3.0642e-05 - val_loss: 3.1780e-05\n",
      "Epoch 23/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 2.9648e-05 - val_loss: 2.7484e-05\n",
      "Epoch 24/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 3.0124e-05 - val_loss: 2.6035e-05\n",
      "Epoch 25/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 2.8816e-05 - val_loss: 2.5329e-05\n",
      "Epoch 26/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 2.8858e-05 - val_loss: 2.9652e-05\n",
      "Epoch 27/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 2.8479e-05 - val_loss: 3.0432e-05\n",
      "Epoch 28/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 2.7959e-05 - val_loss: 2.4300e-05\n",
      "Epoch 29/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 2.6728e-05 - val_loss: 2.7708e-05\n",
      "Epoch 30/30\n",
      "973/973 [==============================] - 1s 1ms/step - loss: 2.6217e-05 - val_loss: 3.4098e-05\n",
      "3.287E-03\n",
      "2.918E-02\n",
      "9.646E-06\n",
      "model time = 35.16395385400028\n"
     ]
    }
   ],
   "source": [
    "startTime = timeit.default_timer()\n",
    "up_bound = 1.0\n",
    "low_bound = 0.0\n",
    "points = 250\n",
    "grid = np.linspace(low_bound, up_bound, num=points)\n",
    "Intervals, FInterval = IntervalsRandom(points, grid)\n",
    "x_val, y_val = ValidationFixed(width, low_bound, up_bound)\n",
    "model = TrainModelArbitrary(Intervals, FInterval, x_val, y_val)\n",
    "\n",
    "relError, IntervalTest, FTest = TestModelArbitrary(points, low_bound, up_bound,  model)\n",
    "\n",
    "\n",
    "for i in range(len(relError)):\n",
    "    relError[i] = '%.3E' % Decimal(relError[i])\n",
    "print( '%.3E' % Decimal(relError.sum()/len(relError)))\n",
    "print(  '%.3E' % Decimal(max(relError)))\n",
    "print( '%.3E' % Decimal(min(relError)))\n",
    "\n",
    "modelTime = timeit.default_timer() - startTime\n",
    "print(\"model time =\", modelTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same problem tested on LSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0086 - val_loss: 0.0033\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 9.4026e-04 - val_loss: 3.0603e-04\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 3.4883e-04 - val_loss: 3.2512e-04\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 3.3860e-04 - val_loss: 2.9179e-04\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 3.2866e-04 - val_loss: 3.2001e-04\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 3.1839e-04 - val_loss: 2.9153e-04\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 3.0821e-04 - val_loss: 2.7724e-04\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.9728e-04 - val_loss: 2.7634e-04\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.8653e-04 - val_loss: 2.5885e-04\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.7655e-04 - val_loss: 2.6211e-04\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.6675e-04 - val_loss: 2.2387e-04\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.5645e-04 - val_loss: 2.3012e-04\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.4673e-04 - val_loss: 1.9962e-04\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3713e-04 - val_loss: 2.1888e-04\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.2631e-04 - val_loss: 1.8313e-04\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.1441e-04 - val_loss: 1.9462e-04\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0233e-04 - val_loss: 1.9262e-04\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.8905e-04 - val_loss: 1.6237e-04\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.7451e-04 - val_loss: 1.4205e-04\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.5906e-04 - val_loss: 1.2255e-04\n",
      "1.295E-02\n",
      "4.283E-02\n",
      "9.084E-06\n",
      "model time = 17.525063234999834\n"
     ]
    }
   ],
   "source": [
    "startTime = timeit.default_timer()\n",
    "up_bound = 1.0\n",
    "low_bound = 0.0\n",
    "numIntervals = 10000\n",
    "#grid = np.linspace(low_bound, up_bound, num=points)\n",
    "\n",
    "Intervals, FInterval = IntervalsRandomEven(numIntervals, width, low_bound, up_bound)\n",
    "IntervalsR = np.reshape(Intervals, (Intervals.shape[0], 1, Intervals.shape[1]))\n",
    "FIntervalR = np.reshape(FInterval, (FInterval.shape[0], 1, 1))\n",
    "\n",
    "x_val, y_val = ValidationFixed(width, low_bound, up_bound)\n",
    "x_valR = np.reshape(x_val, (x_val.shape[0], 1, x_val.shape[1]))\n",
    "y_valR = np.reshape(y_val, (y_val.shape[0], 1, 1))\n",
    "\n",
    "model = TrainModelZeroRecurrent(IntervalsR, FIntervalR, x_valR, y_valR)\n",
    "\n",
    "relError, IntervalTest, FTest = TestModelRecurrentEven(width, low_bound, up_bound, model)\n",
    "relError = np.asarray(relError)\n",
    "for i in range(len(relError)):\n",
    "    relError[i] = '%.3E' % Decimal(relError[i])\n",
    "print( '%.3E' % Decimal(relError.sum()/len(relError)))\n",
    "print(  '%.3E' % Decimal(max(relError)))\n",
    "print( '%.3E' % Decimal(min(relError)))\n",
    "\n",
    "modelTime = timeit.default_timer() - startTime\n",
    "print(\"model time =\", modelTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part2. \n",
    "In the next few cells rectangles on [0,2] x [0,2] are generated and tested on rectangles of arbitary sizes on\n",
    "[0,2] x [0,2]. First, number of points in the x and y direction are specified. This results in 20x20 grid. \n",
    "Subsequently, all possible combinations or rectangles on that grid is generated and used as a training set. \n",
    "After that, similar analysis to the previous problems is performed on Dense networks and LSTM+Dense networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdb60683490>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#generate grid an plot it\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "up_bound = 2.0\n",
    "low_bound = 0.0\n",
    "pointsX = 15\n",
    "pointsY = 15\n",
    "points = pointsX#since assuming domain is a square\n",
    "numPoints = pointsX * pointsY\n",
    "gridX = np.linspace(low_bound, up_bound, num=pointsX)\n",
    "gridY = np.linspace(low_bound, up_bound, num=pointsY)\n",
    "\n",
    "vert = np.zeros([numPoints, 2])\n",
    "for i in range(pointsX):\n",
    "    for j in range(pointsY):\n",
    "        vert[i*pointsY + j, 0] = gridX[i]\n",
    "        vert[i*pointsY + j, 1] = gridY[j]\n",
    "        \n",
    "count = 0\n",
    "vertX = []\n",
    "vertY = []\n",
    "for i in range(numPoints):\n",
    "    vertX.append(vert[i, 0])\n",
    "    vertY.append(vert[i, 1])\n",
    "plt.plot(vertX, vertY, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the segment of code in this cell generates all possible combinations of rectangles on the grid\n",
    "#def GenerateGrid:\n",
    "\n",
    "numBoundVert = (pointsX-1) * (pointsY-1)\n",
    "vertLeftDown = np.zeros([numBoundVert, 2])\n",
    "vertRightUp = np.zeros([numBoundVert, 2])\n",
    "\n",
    "for i in range(pointsX-1):\n",
    "    for j in range(pointsY-1):\n",
    "        vertLeftDown[i*(pointsY-1) + j, :] = vert[i*pointsY +j, :]\n",
    "        vertRightUp[i*(pointsY-1) + j, :] = vert[(i+1)*pointsY + (j+1), :]\n",
    "      \n",
    "#formula for calculating number of rectangles in a grid from \n",
    "#https://www.geeksforgeeks.org/number-rectangles-nm-grid\n",
    "#with slight adjustment based on how I am indexing\n",
    "numSquares = (pointsX-1)*(pointsX) * (pointsY-1)*(pointsY) / 4 \n",
    "numSquares = int(numSquares)\n",
    "Squares = np.zeros([numSquares, 8])\n",
    "\n",
    "outcount = 0\n",
    "inCount = 0\n",
    "totCount = 0\n",
    "for i in range (numBoundVert):\n",
    "    inCount = 0\n",
    "    j  = i\n",
    "    while(j < numBoundVert):\n",
    "        \n",
    "        Squares[totCount, 0] = vertLeftDown[i, 0]\n",
    "        Squares[totCount, 1] = vertLeftDown[i, 1]  \n",
    "        Squares[totCount, 2] = vertRightUp[j, 0]\n",
    "        Squares[totCount, 3] = vertLeftDown[i, 1] \n",
    "        Squares[totCount, 4] = vertLeftDown[i, 0]\n",
    "        Squares[totCount, 5] = vertRightUp[j, 1]  \n",
    "        Squares[totCount, 6] = vertRightUp[j,0]\n",
    "        Squares[totCount, 7] = vertRightUp[j,1]\n",
    "        \n",
    "        totCount += 1\n",
    "        inCount += 1\n",
    "\n",
    "        if (( (j+1) % (pointsY-1)) == 0):\n",
    "            j = j + (outcount)%(pointsX-1)\n",
    "           \n",
    "            j = j+1\n",
    "            continue \n",
    "            \n",
    "        j = j+1\n",
    "        \n",
    "    outcount += 1\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-b70c173a9390>:18: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = fig.add_subplot(111)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#in this cell some generated rectangles are plotted\n",
    "#Snippet of the code below from from https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "N = 15\n",
    "cmap = get_cmap(N)\n",
    "\n",
    "\n",
    "#reference for plotting patches https://www.geeksforgeeks.org/matplotlib-patches-rectangle-in-python/\n",
    "from matplotlib.patches import Rectangle\n",
    "  \n",
    "\n",
    "fig = plt.figure()\n",
    "plt.xlim(low_bound, up_bound)\n",
    "plt.ylim(low_bound, up_bound)\n",
    "ax = fig.add_subplot(111)\n",
    "count = 0\n",
    "r = [18, 200,  600, 1500, 2023]\n",
    "for i in range(5):\n",
    "    \n",
    "    ax.add_patch( Rectangle((Squares[r[i],0:2]),\n",
    "                        Squares[r[i],6]-Squares[r[i],0], Squares[r[i], 7]-Squares[r[i],1],\n",
    "                        fc ='none', \n",
    "                        color = cmap(count)))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute targets. In this instance: integral of a function exp(x+y) over generated rectangles\n",
    "F = np.zeros(numSquares)\n",
    "for i in range(numSquares):\n",
    "    F[i] = ( np.exp(Squares[i,6]) - np.exp(Squares[i,0]) ) * (np.exp(Squares[i,7]) - np.exp(Squares[i,1]) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1129/1129 [==============================] - 2s 1ms/step - loss: 1.3529\n",
      "Epoch 2/30\n",
      "1129/1129 [==============================] - 2s 1ms/step - loss: 0.7379\n",
      "Epoch 3/30\n",
      "1129/1129 [==============================] - 1s 1ms/step - loss: 0.4369\n",
      "Epoch 4/30\n",
      "1129/1129 [==============================] - 2s 1ms/step - loss: 0.3034\n",
      "Epoch 5/30\n",
      "1129/1129 [==============================] - 2s 2ms/step - loss: 0.2383\n",
      "Epoch 6/30\n",
      "1129/1129 [==============================] - 2s 2ms/step - loss: 0.2009\n",
      "Epoch 7/30\n",
      "1129/1129 [==============================] - 2s 1ms/step - loss: 0.1697\n",
      "Epoch 8/30\n",
      "1129/1129 [==============================] - 2s 1ms/step - loss: 0.1419\n",
      "Epoch 9/30\n",
      "1129/1129 [==============================] - 2s 1ms/step - loss: 0.1196\n",
      "Epoch 10/30\n",
      "1129/1129 [==============================] - 2s 1ms/step - loss: 0.1034\n",
      "Epoch 11/30\n",
      "1129/1129 [==============================] - 2s 1ms/step - loss: 0.0913\n",
      "Epoch 12/30\n",
      "1129/1129 [==============================] - 2s 1ms/step - loss: 0.0814\n",
      "Epoch 13/30\n",
      "1129/1129 [==============================] - 2s 1ms/step - loss: 0.0751\n",
      "Epoch 14/30\n",
      "1129/1129 [==============================] - 2s 2ms/step - loss: 0.0690\n",
      "Epoch 15/30\n",
      "1129/1129 [==============================] - 2s 1ms/step - loss: 0.0650\n",
      "Epoch 16/30\n",
      "1129/1129 [==============================] - 1s 1ms/step - loss: 0.0613\n",
      "Epoch 17/30\n",
      "1129/1129 [==============================] - 1s 1ms/step - loss: 0.0579\n",
      "Epoch 18/30\n",
      "1129/1129 [==============================] - 1s 1ms/step - loss: 0.0553\n",
      "Epoch 19/30\n",
      "1129/1129 [==============================] - 1s 1ms/step - loss: 0.0532\n",
      "Epoch 20/30\n",
      "1129/1129 [==============================] - 1s 1ms/step - loss: 0.0511\n",
      "Epoch 21/30\n",
      "1129/1129 [==============================] - 1s 1ms/step - loss: 0.0486\n",
      "Epoch 22/30\n",
      "1129/1129 [==============================] - 1s 1ms/step - loss: 0.0468\n",
      "Epoch 23/30\n",
      "1129/1129 [==============================] - 1s 1ms/step - loss: 0.0452\n",
      "Epoch 24/30\n",
      "1129/1129 [==============================] - 1s 1ms/step - loss: 0.0426\n",
      "Epoch 25/30\n",
      "1129/1129 [==============================] - 1s 1ms/step - loss: 0.0418\n",
      "Epoch 26/30\n",
      "1129/1129 [==============================] - 1s 1ms/step - loss: 0.0402\n",
      "Epoch 27/30\n",
      "1129/1129 [==============================] - 2s 2ms/step - loss: 0.0392\n",
      "Epoch 28/30\n",
      "1129/1129 [==============================] - 2s 1ms/step - loss: 0.0382\n",
      "Epoch 29/30\n",
      "1129/1129 [==============================] - 2s 1ms/step - loss: 0.0367\n",
      "Epoch 30/30\n",
      "1129/1129 [==============================] - 2s 1ms/step - loss: 0.0354\n",
      "5.940E-03\n",
      "2.966E-02\n",
      "7.246E-06\n",
      "model time = 46.562475310999616\n"
     ]
    }
   ],
   "source": [
    "startTime = timeit.default_timer()\n",
    "\n",
    "#train the model\n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import l2\n",
    "from scipy.linalg import norm\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "model = models.Sequential() \n",
    "model.add(layers.Dense(128,  kernel_initializer='normal', activation = 'relu',  input_shape=(8, )))\n",
    "model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1))\n",
    "opt = optimizers.Adam(learning_rate=0.00008)\n",
    "model.compile(loss= Integral_loss, optimizer=opt)\n",
    "model.fit(Squares, F, batch_size = 32,  epochs = 30)\n",
    "  \n",
    "\n",
    "#test on random rectangles \n",
    "import random\n",
    "SquaresTest = np.zeros([1000,8])\n",
    "FTest = np.zeros(1000)\n",
    "relError = np.zeros(1000)\n",
    "absError = np.zeros(1000)\n",
    "\n",
    "i = 0\n",
    "minWidth = (up_bound - low_bound) / pointsX\n",
    "minHeight = (up_bound - low_bound) / pointsY\n",
    "while(i<1000):\n",
    "    x1 = random.uniform(low_bound,up_bound) \n",
    "    y1 = random.uniform(low_bound,up_bound) \n",
    "    x2 = random.uniform(low_bound,up_bound) \n",
    "    y2 = random.uniform(low_bound,up_bound)\n",
    "    xLeft = min(x1,x2)\n",
    "    xRight = max(x1,x2)\n",
    "    yDown = min(y1,y2)\n",
    "    yUp = max(y1, y2)\n",
    "    if( (abs(xRight-xLeft) < 10.0*minWidth) or (abs(yUp-yDown) < 10.0*minHeight) ):\n",
    "        continue\n",
    "    SquaresTest[i,0] = xLeft\n",
    "    SquaresTest[i,1] = yDown\n",
    "    SquaresTest[i,2] = xRight\n",
    "    SquaresTest[i,3] = yDown\n",
    "    SquaresTest[i,4] = xLeft\n",
    "    SquaresTest[i,5] = yUp\n",
    "    SquaresTest[i,6] = xRight\n",
    "    SquaresTest[i,7] = yUp\n",
    "    FTest[i] = ( np.exp(SquaresTest[i,6]) - np.exp(SquaresTest[i,0]) ) * (np.exp(SquaresTest[i,7]) - np.exp(SquaresTest[i,1]) )\n",
    "    i += 1\n",
    "\n",
    "Predicted = model.predict(SquaresTest, verbose=0)\n",
    "for i in range(0,1000):\n",
    "    relError[i] = abs(Predicted[i] - FTest[i]) / abs(FTest[i]) \n",
    "    absError[i] = abs(Predicted[i] - FTest[i])  \n",
    "\n",
    "\n",
    "relError = np.asarray(relError)\n",
    "for i in range(len(relError)):\n",
    "    relError[i] = '%.3E' % Decimal(relError[i])\n",
    "print( '%.3E' % Decimal(relError.sum()/len(relError)))\n",
    "print(  '%.3E' % Decimal(max(relError)))\n",
    "print( '%.3E' % Decimal(min(relError)))\n",
    "\n",
    "modelTime = timeit.default_timer() - startTime\n",
    "print(\"model time =\", modelTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Squares = np.reshape(Squares, (Squares.shape[0], 1, Squares.shape[1]))\n",
    "F = np.reshape(F, (F.shape[0], 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 2.4382\n",
      "Epoch 2/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 1.7979\n",
      "Epoch 3/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 1.3523\n",
      "Epoch 4/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 1.1024\n",
      "Epoch 5/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.9416\n",
      "Epoch 6/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.8259\n",
      "Epoch 7/75\n",
      "345/345 [==============================] - 1s 3ms/step - loss: 0.7137\n",
      "Epoch 8/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.5940\n",
      "Epoch 9/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.4876\n",
      "Epoch 10/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.4048\n",
      "Epoch 11/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.3277\n",
      "Epoch 12/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.2646\n",
      "Epoch 13/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.2205\n",
      "Epoch 14/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.1927\n",
      "Epoch 15/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.1713\n",
      "Epoch 16/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.1551\n",
      "Epoch 17/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.1417\n",
      "Epoch 18/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.1288\n",
      "Epoch 19/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.1209\n",
      "Epoch 20/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.1112\n",
      "Epoch 21/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.1076\n",
      "Epoch 22/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.1001\n",
      "Epoch 23/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0921\n",
      "Epoch 24/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0890\n",
      "Epoch 25/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0816\n",
      "Epoch 26/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0791\n",
      "Epoch 27/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0745\n",
      "Epoch 28/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0719\n",
      "Epoch 29/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0681\n",
      "Epoch 30/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0666\n",
      "Epoch 31/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0622\n",
      "Epoch 32/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0604\n",
      "Epoch 33/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0567\n",
      "Epoch 34/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0572\n",
      "Epoch 35/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0539\n",
      "Epoch 36/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0547\n",
      "Epoch 37/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0524\n",
      "Epoch 38/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0530\n",
      "Epoch 39/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0498\n",
      "Epoch 40/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0477\n",
      "Epoch 41/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0467\n",
      "Epoch 42/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0480\n",
      "Epoch 43/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0455\n",
      "Epoch 44/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0464\n",
      "Epoch 45/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0446\n",
      "Epoch 46/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0433\n",
      "Epoch 47/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0441\n",
      "Epoch 48/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0427\n",
      "Epoch 49/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0423\n",
      "Epoch 50/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0404\n",
      "Epoch 51/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0404\n",
      "Epoch 52/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0391\n",
      "Epoch 53/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0401\n",
      "Epoch 54/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0390\n",
      "Epoch 55/75\n",
      "345/345 [==============================] - 2s 4ms/step - loss: 0.0388\n",
      "Epoch 56/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0386\n",
      "Epoch 57/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0385\n",
      "Epoch 58/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0373\n",
      "Epoch 59/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0373\n",
      "Epoch 60/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0371\n",
      "Epoch 61/75\n",
      "345/345 [==============================] - 2s 5ms/step - loss: 0.0354\n",
      "Epoch 62/75\n",
      "345/345 [==============================] - 2s 6ms/step - loss: 0.0362\n",
      "Epoch 63/75\n",
      "345/345 [==============================] - 2s 5ms/step - loss: 0.0365\n",
      "Epoch 64/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0354\n",
      "Epoch 65/75\n",
      "345/345 [==============================] - 2s 4ms/step - loss: 0.0344\n",
      "Epoch 66/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0344\n",
      "Epoch 67/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0343\n",
      "Epoch 68/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0338\n",
      "Epoch 69/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0326\n",
      "Epoch 70/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0325\n",
      "Epoch 71/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0334\n",
      "Epoch 72/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0326\n",
      "Epoch 73/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0329\n",
      "Epoch 74/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0324\n",
      "Epoch 75/75\n",
      "345/345 [==============================] - 1s 4ms/step - loss: 0.0330\n",
      "[4.53638242e-03 3.14526033e-03 5.92542028e-03 5.20782725e-03\n",
      " 6.77536186e-03 6.30330025e-03 2.05731258e-03 3.87687777e-03\n",
      " 4.49203727e-03 2.97956918e-04 8.77839389e-04 8.62845463e-03\n",
      " 3.24981028e-03 6.92284836e-03 6.06504321e-03 4.43842960e-03\n",
      " 7.11041995e-03 4.16836110e-03 3.12495487e-03 8.64590629e-03\n",
      " 2.84481872e-03 6.85177815e-03 4.00985374e-03 4.45279488e-04\n",
      " 2.55934217e-03 7.48334803e-03 3.42694807e-03 3.60295504e-03\n",
      " 2.55392548e-03 2.41688989e-03 7.33742891e-04 5.68286534e-03\n",
      " 1.28611341e-02 9.76687277e-03 2.09405485e-03 9.74388677e-03\n",
      " 9.07292885e-03 1.96786360e-03 3.98842672e-03 4.77327405e-03\n",
      " 3.41339344e-03 3.57349188e-03 1.03894041e-02 4.90296636e-03\n",
      " 5.27037540e-03 2.84912970e-03 9.21604982e-03 2.08145416e-03\n",
      " 6.71164350e-03 4.98390828e-04 6.90805970e-03 7.53230927e-03\n",
      " 8.00339506e-04 1.77558155e-04 7.84823264e-03 1.35689769e-02\n",
      " 3.39060042e-03 6.41015790e-03 4.00964925e-04 3.10001268e-03\n",
      " 4.88910866e-03 9.50770966e-03 1.37633997e-02 3.00417672e-04\n",
      " 1.12178607e-02 5.68458906e-03 4.33868477e-03 8.19893507e-03\n",
      " 4.01558822e-03 1.22399280e-02 2.24342556e-03 3.88421021e-04\n",
      " 3.39685051e-03 1.04354778e-02 1.91088358e-03 5.74745738e-03\n",
      " 3.92014258e-03 7.21752826e-03 2.83053661e-03 3.85320634e-03\n",
      " 4.04617365e-03 9.36823982e-03 7.04618078e-03 1.32367217e-02\n",
      " 1.06034072e-03 1.53268162e-02 1.25135032e-02 1.17799131e-02\n",
      " 5.96641908e-03 2.40165031e-03 1.35890005e-02 2.53845296e-03\n",
      " 7.19801818e-03 3.12000528e-03 1.15187476e-02 1.78720101e-03\n",
      " 1.43864949e-04 2.68747365e-03 1.43678215e-02 1.78065483e-03\n",
      " 9.00365050e-04 4.83311498e-03 8.92899034e-03 4.56462541e-04\n",
      " 2.63438886e-03 8.89496510e-03 4.98085682e-03 3.42670175e-03\n",
      " 1.14263494e-02 5.39333215e-03 1.33330455e-03 1.79930107e-03\n",
      " 2.28767826e-03 2.93625524e-03 1.24277713e-03 2.93521376e-03\n",
      " 8.46738383e-03 1.43014860e-02 3.92391711e-03 1.28419512e-02\n",
      " 5.50594736e-03 1.09630151e-02 5.47750854e-03 3.75906999e-03\n",
      " 7.13244694e-03 3.56712190e-03 5.62086811e-04 6.96858319e-03\n",
      " 1.77745293e-04 6.63417600e-03 2.13747346e-04 1.62898475e-03\n",
      " 9.06815523e-03 9.21687022e-03 1.07496857e-02 6.56983811e-03\n",
      " 1.13185469e-02 7.79434591e-03 8.51800953e-03 2.46511449e-03\n",
      " 1.35973297e-03 5.92544518e-03 1.46826164e-03 1.46030489e-03\n",
      " 1.50897713e-03 3.26766338e-03 1.40382476e-03 9.18314087e-03\n",
      " 1.98465159e-03 8.20742942e-03 1.23749747e-02 1.64318462e-03\n",
      " 1.62296517e-03 3.65024677e-03 8.11864051e-03 2.77427413e-03\n",
      " 6.13970164e-03 4.35787933e-03 4.53786297e-03 2.66246462e-03\n",
      " 6.60944962e-03 1.49833951e-03 4.32759678e-03 1.93244617e-03\n",
      " 1.98736134e-03 8.10528278e-03 5.51079820e-03 4.65863085e-03\n",
      " 1.15074015e-02 1.21596459e-02 1.60520347e-03 3.55194351e-04\n",
      " 4.24601635e-03 4.56718239e-04 9.70763496e-04 1.02557838e-02\n",
      " 2.75288462e-03 3.72929655e-03 3.16901642e-03 5.61671758e-03\n",
      " 1.37184348e-02 5.99277849e-03 6.99686349e-04 5.21528372e-04\n",
      " 4.23537070e-03 1.41733866e-02 3.67619296e-03 1.24095840e-02\n",
      " 7.32253100e-04 5.05256839e-03 1.22554804e-02 2.95753721e-03\n",
      " 1.40218505e-03 7.40597442e-03 2.85860081e-03 7.98461698e-03\n",
      " 9.21091157e-03 5.67538582e-03 3.47256778e-03 6.39320450e-04\n",
      " 1.18522878e-03 1.02676769e-02 5.19882435e-03 7.22139413e-04\n",
      " 8.56963142e-04 8.25310300e-04 5.25279701e-03 4.68325960e-03\n",
      " 5.21753153e-03 6.12398232e-03 2.83552427e-03 6.64461409e-03\n",
      " 6.62150302e-04 4.60096200e-04 4.97720037e-03 8.70959833e-03\n",
      " 2.06466605e-04 5.24081991e-03 2.70761643e-03 3.67858040e-03\n",
      " 8.89103900e-03 9.56885977e-03 1.84116944e-03 1.95745484e-03\n",
      " 1.22727828e-02 4.09958265e-03 3.93292342e-04 5.01861696e-03\n",
      " 6.55534948e-03 6.32535753e-03 4.31637559e-03 1.22960060e-03\n",
      " 9.57380208e-03 1.90058754e-03 4.53191029e-03 9.36553963e-03\n",
      " 1.20713986e-03 3.26079156e-03 5.16257406e-03 1.42456087e-02\n",
      " 2.51769138e-03 3.83395214e-03 4.34065331e-03 1.18922256e-03\n",
      " 7.47348777e-03 2.46428287e-03 1.10218047e-02 2.47782248e-03\n",
      " 1.66408856e-03 3.66669130e-03 1.40753967e-03 2.18846530e-03\n",
      " 1.77060719e-03 5.27603818e-03 2.08619524e-03 9.98827145e-04\n",
      " 4.85859798e-03 2.64048766e-04 1.58875847e-03 3.28277988e-03\n",
      " 2.07243551e-03 7.81130496e-03 4.71551716e-03 3.29258248e-03\n",
      " 2.51748255e-03 3.50547552e-03 2.95706569e-03 2.77712306e-03\n",
      " 8.87229059e-03 7.69816601e-03 5.30507686e-03 9.52006773e-03\n",
      " 9.76322765e-04 7.13246808e-03 1.53814320e-03 6.73770511e-03\n",
      " 4.74407458e-03 2.18150701e-03 1.19961578e-03 6.86898258e-03\n",
      " 4.49313044e-03 1.85046053e-03 6.99272096e-03 1.72736783e-03\n",
      " 1.04319735e-02 9.67519798e-04 3.47065625e-03 1.48747207e-02\n",
      " 5.53179836e-04 5.29846851e-04 1.78756142e-03 6.27953666e-04\n",
      " 7.26887740e-03 1.66137437e-03 1.91203025e-03 7.00815528e-03\n",
      " 1.92050286e-03 1.41221027e-02 3.31989460e-03 9.28506479e-04\n",
      " 4.24387718e-03 6.58865161e-03 9.46716290e-04 1.76268203e-03\n",
      " 1.99888061e-03 1.74303968e-03 8.64362463e-04 5.63984104e-03\n",
      " 5.88706591e-03 6.25928242e-03 4.62743956e-03 8.11507931e-04\n",
      " 3.48812822e-04 7.40016554e-03 2.77774169e-03 1.12278865e-02\n",
      " 2.86637218e-03 5.67552841e-03 6.22484607e-03 4.66966768e-03\n",
      " 2.85303255e-03 3.24614423e-03 1.11045862e-03 2.27108604e-03\n",
      " 6.83137575e-03 6.75365972e-04 2.51386697e-03 1.09378903e-03\n",
      " 2.64964935e-03 1.37326705e-02 1.54132467e-03 7.79681457e-03\n",
      " 1.47069284e-03 1.47259742e-02 5.69779514e-03 6.63003432e-03\n",
      " 1.04886482e-02 5.57195029e-03 1.09694711e-03 6.12794200e-03\n",
      " 9.29786596e-04 3.92720217e-03 1.02877567e-02 4.52199718e-03\n",
      " 7.98869917e-03 2.32060154e-03 1.24844597e-03 1.19567150e-02\n",
      " 2.44764400e-04 9.84452495e-03 6.14580811e-03 2.75560760e-03\n",
      " 3.53553047e-03 2.02693020e-03 6.22156784e-03 5.36186130e-04\n",
      " 1.05777944e-02 6.65167047e-03 5.17304816e-04 2.64624300e-03\n",
      " 4.27900720e-03 2.91338566e-04 9.59429701e-04 3.12166375e-03\n",
      " 8.84373677e-03 1.51442689e-02 8.56244484e-03 1.09545393e-03\n",
      " 7.54477770e-04 7.89930865e-03 2.85312783e-03 1.36771882e-02\n",
      " 1.34425202e-02 1.06716449e-02 1.54062249e-02 2.47486897e-03\n",
      " 3.13262538e-04 2.69469792e-04 7.38472341e-03 1.81572686e-03\n",
      " 1.45775479e-03 6.27228782e-03 1.67976406e-03 3.73065225e-03\n",
      " 9.76003129e-03 4.20282787e-03 2.34164173e-03 5.02403476e-03\n",
      " 7.29756347e-03 7.41243233e-03 3.26129827e-03 3.75150310e-03\n",
      " 4.28988921e-03 1.16537188e-02 7.46293114e-03 5.12768170e-03\n",
      " 1.30434421e-02 3.96558436e-03 6.07169192e-04 2.61555681e-03\n",
      " 2.17953454e-03 1.13433454e-02 2.87428969e-03 2.15348685e-03\n",
      " 7.79530267e-03 2.04157676e-04 5.07623579e-03 7.95493173e-03\n",
      " 6.64592430e-03 3.02614654e-03 7.87395220e-03 3.05113552e-03\n",
      " 1.14971869e-02 1.18028063e-02 4.65061535e-03 3.50914039e-04\n",
      " 3.09126121e-03 6.92777316e-03 3.72368455e-03 6.20759370e-03\n",
      " 2.55919715e-03 2.68723574e-03 5.83441630e-05 1.12534339e-03\n",
      " 9.53893722e-03 5.35546115e-03 7.79003393e-03 5.62016141e-03\n",
      " 3.15610329e-03 1.17009664e-02 1.36522012e-03 3.97997922e-03\n",
      " 2.54265556e-04 2.48710590e-03 2.61721645e-03 1.72015483e-04\n",
      " 3.95532164e-03 5.27754216e-03 4.58894836e-03 1.42288879e-02\n",
      " 9.56490434e-03 9.55788325e-04 4.01062070e-04 3.08662009e-03\n",
      " 1.41723431e-03 5.13950181e-04 4.54609774e-03 4.46186944e-03\n",
      " 5.21550267e-03 6.43855107e-03 2.16703025e-03 5.28040079e-03\n",
      " 1.99560122e-03 6.11458261e-03 9.90800280e-03 7.04917680e-03\n",
      " 6.85320869e-03 7.13661822e-03 7.84616950e-03 5.14608490e-03\n",
      " 3.21151173e-03 1.20545323e-02 1.00351867e-02 6.32180533e-04\n",
      " 3.13346660e-03 1.77078175e-03 4.35606002e-03 2.20798512e-03\n",
      " 4.54745450e-03 1.14881406e-02 1.69136214e-03 2.13783593e-04\n",
      " 1.03611049e-02 3.43004546e-03 3.86158355e-03 9.84086942e-03\n",
      " 5.62877376e-04 3.07871132e-03 5.18525105e-04 5.08234162e-03\n",
      " 7.04906285e-03 1.69261244e-03 3.97195604e-03 1.34178213e-02\n",
      " 2.41812519e-03 1.30967283e-03 1.19155879e-03 5.07994711e-04\n",
      " 2.10207377e-03 2.70041664e-03 4.72400082e-03 6.86328758e-04\n",
      " 4.04441567e-04 1.02515781e-02 6.45825055e-03 3.21735868e-03\n",
      " 1.51905865e-02 1.19602268e-04 8.75569178e-04 4.35253615e-03\n",
      " 1.22631302e-03 1.84557734e-03 8.22365773e-03 1.43020710e-03\n",
      " 4.36473515e-03 4.52441125e-03 1.20915631e-02 1.42527200e-02\n",
      " 4.01661533e-03 2.25764379e-03 1.11347498e-03 5.01286958e-03\n",
      " 5.62309786e-03 6.91049562e-04 4.33371027e-03 4.56283552e-03\n",
      " 6.72737035e-03 5.02316205e-03 3.62957501e-04 4.29778759e-03\n",
      " 2.84402991e-04 3.22253451e-03 7.20898179e-03 3.83777489e-03\n",
      " 9.47685286e-03 4.41364007e-03 4.72610075e-03 9.50423092e-03\n",
      " 3.48576546e-03 6.34686493e-05 2.02285729e-03 4.41317662e-03\n",
      " 5.09098980e-03 5.05682921e-04 2.13020145e-03 3.91968779e-03\n",
      " 1.08367518e-03 8.18426378e-03 2.53995937e-03 8.55987532e-03\n",
      " 1.69585808e-03 5.67684102e-03 1.31286976e-02 4.32236751e-03\n",
      " 5.39423846e-03 3.59708024e-03 4.56151056e-04 1.42807918e-03\n",
      " 6.70844457e-03 3.30030578e-03 1.17263629e-03 2.11967990e-03\n",
      " 3.33518211e-03 6.69302863e-03 2.87406682e-03 7.14193007e-03\n",
      " 2.08478411e-03 1.18870081e-03 2.03953544e-03 8.72269730e-03\n",
      " 1.06153704e-02 2.36825636e-03 7.29339261e-03 3.72564987e-03\n",
      " 3.78204675e-03 4.46092933e-03 2.43753503e-03 6.49803809e-04\n",
      " 1.73202573e-03 1.70933073e-03 5.82041883e-03 8.97576992e-04\n",
      " 5.66420913e-03 1.82781636e-03 7.43606027e-03 4.30423161e-03\n",
      " 7.15392714e-03 4.47053362e-03 8.45388269e-03 1.15575774e-02\n",
      " 7.06799789e-03 4.03846510e-04 4.09703346e-03 2.88657442e-03\n",
      " 9.68018622e-05 3.60302501e-03 3.16055097e-03 3.88796136e-03\n",
      " 2.89036602e-03 7.24672380e-03 5.46733629e-03 1.08959570e-02\n",
      " 1.36291642e-02 1.91461753e-03 3.59420508e-03 6.82249637e-03\n",
      " 7.03628058e-03 5.85405614e-03 6.53518328e-03 4.12690380e-03\n",
      " 2.29414811e-03 5.09293681e-03 2.06476251e-03 1.44450888e-03\n",
      " 2.56216292e-03 4.88734058e-03 4.36343221e-03 6.88277718e-03\n",
      " 3.41604290e-03 2.82096634e-03 4.35668360e-03 2.53565319e-04\n",
      " 2.50627991e-03 6.67415407e-03 5.58214909e-03 3.30709055e-03\n",
      " 3.68710000e-03 1.26888837e-02 1.54760874e-03 1.26520276e-02\n",
      " 3.70078579e-03 1.76017181e-03 3.85068103e-04 5.63410743e-03\n",
      " 1.29807394e-02 1.14277147e-03 4.29570837e-03 5.99240036e-03\n",
      " 5.62374822e-03 1.73256832e-03 2.28160342e-03 4.11098274e-03\n",
      " 1.84700601e-03 8.48009375e-03 3.22869625e-04 6.10090281e-03\n",
      " 5.71651644e-03 5.00683979e-03 3.39200570e-03 1.07277578e-03\n",
      " 1.01717711e-03 3.74679898e-03 2.99803898e-03 1.36960744e-02\n",
      " 2.47942436e-03 1.35449118e-03 3.63807067e-03 1.04369426e-04\n",
      " 2.57046876e-04 1.85466175e-03 1.12046230e-02 2.32543217e-03\n",
      " 2.57969530e-03 1.89935635e-03 5.94428241e-03 2.77821926e-03\n",
      " 2.77635527e-03 5.56193163e-03 4.17779332e-04 2.43558881e-03\n",
      " 2.62050896e-03 1.94893693e-03 1.30019354e-03 3.83805382e-03\n",
      " 1.02850879e-02 1.27274845e-02 2.48780913e-03 6.97728251e-03\n",
      " 4.18379296e-03 1.26282329e-04 8.15605232e-03 1.97403090e-03\n",
      " 2.24557402e-03 4.71599948e-03 5.50568812e-03 7.06463437e-03\n",
      " 2.18244271e-03 4.14097052e-03 7.45058069e-04 3.94798308e-04\n",
      " 4.52981430e-04 3.14919452e-04 1.08346545e-05 7.40292849e-03\n",
      " 7.39223447e-03 7.41653430e-04 4.48066057e-03 1.11126652e-02\n",
      " 1.37122224e-03 1.14171300e-02 1.86156191e-03 3.41591331e-03\n",
      " 6.95829139e-04 1.37831495e-03 5.70662209e-03 6.38628281e-03\n",
      " 3.53904077e-03 1.37118659e-02 1.27970413e-02 2.38161830e-03\n",
      " 4.49274150e-03 5.89542573e-03 7.17125566e-03 3.25567937e-03\n",
      " 4.07509244e-03 8.71151322e-03 2.76401640e-03 3.70891086e-03\n",
      " 9.59448906e-03 6.74077797e-03 4.94380953e-03 3.36039722e-03\n",
      " 1.27788427e-02 5.12008325e-03 6.55517717e-03 2.71236680e-03\n",
      " 2.58402215e-03 4.81579041e-03 1.36857007e-02 1.01329703e-02\n",
      " 9.90793147e-03 3.58083563e-04 3.14074253e-03 3.11698387e-04\n",
      " 1.34152995e-02 6.88598415e-03 9.83873398e-03 1.47421537e-02\n",
      " 5.80336191e-03 5.10183212e-03 1.34645339e-03 2.45675059e-03\n",
      " 5.94053663e-03 1.56309788e-02 7.14589975e-03 3.55160699e-03\n",
      " 4.10022203e-03 2.66546952e-03 5.73956238e-03 1.13412190e-03\n",
      " 4.45732219e-04 1.79593250e-03 1.94962953e-03 2.55202817e-03\n",
      " 6.24118640e-03 5.91963150e-03 7.26035853e-04 6.63463583e-04\n",
      " 7.90862872e-05 2.28089524e-03 7.90222955e-03 1.07785998e-02\n",
      " 8.20642309e-03 1.30679014e-02 1.30513723e-02 4.76496615e-03\n",
      " 5.85785509e-03 1.11432783e-02 5.90535652e-04 7.64422031e-03\n",
      " 2.87698756e-04 1.07352986e-02 6.40322439e-03 1.00716800e-03\n",
      " 1.34526990e-03 2.86780122e-03 6.24837167e-03 1.12360660e-02\n",
      " 4.27469819e-03 1.02442822e-03 2.56202264e-03 3.37503436e-03\n",
      " 1.22929476e-03 6.80760019e-03 4.17507303e-03 2.77237046e-03\n",
      " 9.27081544e-04 1.17038989e-02 5.57003742e-04 2.81334740e-04\n",
      " 1.30258065e-02 3.06836758e-03 3.66547396e-03 1.94939722e-03\n",
      " 6.43127399e-03 5.56789602e-03 1.09748878e-02 1.97547059e-04\n",
      " 1.03623839e-02 3.74898550e-03 6.76521597e-03 1.17222358e-02\n",
      " 1.07123312e-05 1.75589034e-03 3.30438473e-03 1.65893521e-03\n",
      " 1.63849932e-03 3.74229111e-03 8.08656251e-03 5.07231102e-03\n",
      " 3.45555476e-03 3.97060797e-03 7.86882124e-03 4.65670376e-03\n",
      " 4.22909263e-03 1.12957914e-03 7.49260833e-03 3.34805592e-03\n",
      " 1.10980912e-03 2.34155228e-03 6.70964361e-03 5.58657129e-05\n",
      " 9.87827700e-04 5.36342971e-03 2.67891772e-03 1.37102890e-04\n",
      " 5.38540756e-03 2.81446793e-03 6.85677761e-03 9.00415668e-03\n",
      " 6.49836278e-03 5.34512719e-03 8.19195620e-03 6.39373723e-03\n",
      " 3.93452754e-03 1.15550185e-03 7.37164336e-04 3.55920317e-03\n",
      " 7.73990136e-04 1.09226433e-03 1.01387090e-02 4.97485993e-03\n",
      " 5.98440226e-04 1.51969806e-03 5.91862477e-03 2.88173399e-03\n",
      " 2.17892076e-03 6.65155113e-03 7.46791951e-03 3.80815951e-03\n",
      " 3.03849999e-03 1.09572621e-03 3.83957404e-03 4.83962427e-04\n",
      " 3.14261918e-03 1.04785201e-03 1.03492103e-02 5.72796129e-03\n",
      " 1.15282864e-02 2.06590309e-03 7.40898526e-03 7.38128228e-03\n",
      " 4.28424747e-03 3.44745801e-03 7.50366299e-03 1.07304991e-03\n",
      " 2.67416368e-03 1.91624344e-05 2.85248429e-03 1.19965478e-02\n",
      " 3.38888107e-03 1.59939777e-02 8.31253874e-03 5.15626233e-03\n",
      " 2.31862650e-03 2.45204028e-03 1.40491759e-03 2.41611954e-03\n",
      " 8.43318468e-03 8.26064819e-04 7.17618984e-03 5.68359900e-03\n",
      " 8.52211196e-04 5.76940585e-03 2.05500012e-03 1.60527994e-03\n",
      " 6.86468237e-03 4.79503528e-03 5.63567041e-03 3.84181970e-04\n",
      " 6.19259020e-03 3.63161469e-03 2.98329538e-04 2.13371548e-03\n",
      " 1.63925769e-03 4.32128203e-03 3.36950954e-03 1.16787021e-03\n",
      " 2.77802422e-03 3.46926194e-03 9.87987062e-03 3.25557463e-03\n",
      " 3.27868133e-03 1.05691199e-03 9.70952270e-05 4.06652459e-03\n",
      " 6.73836179e-03 5.73322134e-03 5.54110863e-03 1.09386305e-02\n",
      " 3.94925242e-03 1.81003599e-03 3.97745790e-03 6.43046140e-03\n",
      " 1.07177550e-02 5.55004625e-03 5.31607568e-03 9.58356037e-03\n",
      " 1.65186388e-03 2.15929401e-03 2.33915828e-03 9.12570886e-03\n",
      " 5.07746755e-03 1.59166653e-03 2.78534117e-03 4.25309004e-03\n",
      " 4.74155090e-03 5.05128989e-03 6.01534856e-03 1.24417462e-02\n",
      " 3.42911832e-03 3.30017893e-03 4.51357316e-03 5.28788804e-03\n",
      " 1.67640513e-03 3.88887497e-03 4.66996748e-03 1.12812439e-02\n",
      " 1.25313325e-02 4.93675817e-03 3.51419028e-03 2.79327739e-03\n",
      " 6.17446829e-03 2.35215223e-03 8.05453016e-04 1.21261612e-04\n",
      " 1.85996618e-03 2.82956714e-03 3.64555257e-03 3.75575593e-03\n",
      " 1.12144937e-02 3.26786996e-03 7.46208133e-03 3.39800430e-03\n",
      " 4.75479076e-04 5.32690984e-03 4.62567746e-03 9.37729054e-03\n",
      " 1.39100721e-02 1.12224528e-02 2.61476628e-03 2.76055651e-03\n",
      " 7.94827803e-04 5.55977942e-03 8.02910701e-04 1.80302384e-03\n",
      " 4.41997652e-04 6.55926535e-03 1.08458400e-02 9.75435521e-03\n",
      " 5.56900141e-03 2.45967507e-03 3.44118524e-03 1.19520861e-02\n",
      " 1.22380034e-03 1.19157157e-02 4.10335832e-03 6.19295977e-03\n",
      " 4.77722392e-03 6.61884604e-03 8.23803552e-03 2.12882677e-03\n",
      " 6.19172619e-03 4.12259291e-03 5.12052806e-03 1.54978344e-03\n",
      " 3.87658671e-03 2.73823388e-03 2.37166661e-03 6.29700568e-03\n",
      " 3.99374677e-03 1.48507843e-03 4.95039298e-03 3.34297645e-03\n",
      " 1.32518936e-03 9.51740471e-03 8.97254851e-03 5.20619957e-03\n",
      " 9.22306758e-04 2.35573714e-03 4.02377473e-03 1.99889620e-03\n",
      " 5.88049162e-03 1.21340255e-02 2.06951561e-03 7.06449665e-04\n",
      " 1.26597003e-03 3.21903831e-05 3.03800749e-03 1.11001279e-03]\n",
      "4.769E-03\n",
      "1.599E-02\n",
      "1.071E-05\n",
      "model time = 103.11431255499974\n"
     ]
    }
   ],
   "source": [
    "#based on  templates from https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "#and https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "startTime = timeit.default_timer()\n",
    "model = Sequential()\n",
    "model.add( tf.keras.layers.Bidirectional(LSTM(128, input_shape=(1, 8))) ) \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "opt = optimizers.Adam(learning_rate=0.00008)\n",
    "model.compile(loss= Integral_loss, optimizer = opt)\n",
    "model.fit(Squares, F, epochs=75, batch_size=32, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "#test on random rectangles \n",
    "import random\n",
    "SquaresTest = np.zeros([1000,8])\n",
    "FTest = np.zeros(1000)\n",
    "relError = np.zeros(1000)\n",
    "absError = np.zeros(1000)\n",
    "\n",
    "i = 0\n",
    "minWidth = (up_bound - low_bound) / pointsX\n",
    "minHeight = (up_bound - low_bound) / pointsY\n",
    "while(i<1000):\n",
    "    x1 = random.uniform(0,2) \n",
    "    y1 = random.uniform(0,2) \n",
    "    x2 = random.uniform(0,2) \n",
    "    y2 = random.uniform(0,2)\n",
    "    xLeft = min(x1,x2)\n",
    "    xRight = max(x1,x2)\n",
    "    yDown = min(y1,y2)\n",
    "    yUp = max(y1, y2)\n",
    "    if( (abs(xRight-xLeft) < 10*minWidth) or (abs(yUp-yDown) < 10*minHeight) ):\n",
    "        continue\n",
    "    SquaresTest[i,0] = xLeft\n",
    "    SquaresTest[i,1] = yDown\n",
    "    SquaresTest[i,2] = xRight\n",
    "    SquaresTest[i,3] = yDown\n",
    "    SquaresTest[i,4] = xLeft\n",
    "    SquaresTest[i,5] = yUp\n",
    "    SquaresTest[i,6] = xRight\n",
    "    SquaresTest[i,7] = yUp\n",
    "    FTest[i] = ( np.exp(SquaresTest[i,6]) - np.exp(SquaresTest[i,0]) ) * (np.exp(SquaresTest[i,7]) - np.exp(SquaresTest[i,1]) )\n",
    "    i += 1\n",
    "\n",
    "SquaresTest = np.reshape(SquaresTest, (SquaresTest.shape[0], 1, SquaresTest.shape[1]))\n",
    "FTest = np.reshape(FTest, (FTest.shape[0], 1, 1))    \n",
    "Predicted = model.predict(SquaresTest, verbose=0)\n",
    "for i in range(0,1000):\n",
    "    relError[i] = abs(Predicted[i] - FTest[i]) / abs(FTest[i]) \n",
    "    absError[i] = abs(Predicted[i] - FTest[i])  \n",
    "\n",
    "print(relError)\n",
    "\n",
    "\n",
    "relError = np.asarray(relError)\n",
    "for i in range(len(relError)):\n",
    "    relError[i] = '%.3E' % Decimal(relError[i])\n",
    "print( '%.3E' % Decimal(relError.sum()/len(relError)))\n",
    "print(  '%.3E' % Decimal(max(relError)))\n",
    "print( '%.3E' % Decimal(min(relError)))\n",
    "\n",
    "modelTime = timeit.default_timer() - startTime\n",
    "print(\"model time =\", modelTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
